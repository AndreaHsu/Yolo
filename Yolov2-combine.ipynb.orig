{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 28,
>>>>>>> 7763e36cb6e79f4149f5793a02154d2b7caa5931
   "id": "666081b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "import os.path\n",
    "import random\n",
    "from random import uniform\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import math\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42b049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 好神秘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe51cb6",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 29,
>>>>>>> 7763e36cb6e79f4149f5793a02154d2b7caa5931
   "id": "389d3f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compose(object):\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, data):\n",
    "        for function_ in self.transforms:\n",
    "            data = function_(data)\n",
    "        return data\n",
    "\n",
    "\n",
    "class Crop(object):\n",
    "\n",
    "    def __init__(self, max_crop=0.1):\n",
    "        super().__init__()\n",
    "        self.max_crop = max_crop\n",
    "\n",
    "    def __call__(self, data):\n",
    "        image, label = data\n",
    "        height, width = image.shape[:2]\n",
    "        xmin = width\n",
    "        ymin = height\n",
    "        xmax = 0\n",
    "        ymax = 0\n",
    "        for lb in label:\n",
    "            xmin = min(xmin, lb[0])\n",
    "            ymin = min(ymin, lb[1])\n",
    "            xmax = max(xmax, lb[2])\n",
    "            ymax = max(ymax, lb[2])\n",
    "        cropped_left = uniform(0, self.max_crop)\n",
    "        cropped_right = uniform(0, self.max_crop)\n",
    "        cropped_top = uniform(0, self.max_crop)\n",
    "        cropped_bottom = uniform(0, self.max_crop)\n",
    "        new_xmin = int(min(cropped_left * width, xmin))\n",
    "        new_ymin = int(min(cropped_top * height, ymin))\n",
    "        new_xmax = int(max(width - 1 - cropped_right * width, xmax))\n",
    "        new_ymax = int(max(height - 1 - cropped_bottom * height, ymax))\n",
    "\n",
    "        image = image[new_ymin:new_ymax, new_xmin:new_xmax, :]\n",
    "        label = [[lb[0] - new_xmin, lb[1] - new_ymin, lb[2] - new_xmin, lb[3] - new_ymin, lb[4]] for lb in label]\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class VerticalFlip(object):\n",
    "\n",
    "    def __init__(self, prob=0.5):\n",
    "        super().__init__()\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, data):\n",
    "        image, label = data\n",
    "        if uniform(0, 1) >= self.prob:\n",
    "            image = cv2.flip(image, 1)\n",
    "            width = image.shape[1]\n",
    "            label = [[width - lb[2], lb[1], width - lb[0], lb[3], lb[4]] for lb in label]\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class HSVAdjust(object):\n",
    "\n",
    "    def __init__(self, hue=30, saturation=1.5, value=1.5, prob=0.5):\n",
    "        super().__init__()\n",
    "        self.hue = hue\n",
    "        self.saturation = saturation\n",
    "        self.value = value\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, data):\n",
    "\n",
    "        def clip_hue(hue_channel):\n",
    "            hue_channel[hue_channel >= 360] -= 360\n",
    "            hue_channel[hue_channel < 0] += 360\n",
    "            return hue_channel\n",
    "\n",
    "        image, label = data\n",
    "        adjust_hue = uniform(-self.hue, self.hue)\n",
    "        adjust_saturation = uniform(1, self.saturation)\n",
    "        if uniform(0, 1) >= self.prob:\n",
    "            adjust_saturation = 1 / adjust_saturation\n",
    "        adjust_value = uniform(1, self.value)\n",
    "        if uniform(0, 1) >= self.prob:\n",
    "            adjust_value = 1 / adjust_value\n",
    "        image = image.astype(np.float32) / 255\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "        image[:, :, 0] += adjust_hue\n",
    "        image[:, :, 0] = clip_hue(image[:, :, 0])\n",
    "        image[:, :, 1] = np.clip(adjust_saturation * image[:, :, 1], 0.0, 1.0)\n",
    "        image[:, :, 2] = np.clip(adjust_value * image[:, :, 2], 0.0, 1.0)\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_HSV2RGB)\n",
    "        image = (image * 255).astype(np.float32)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class Resize(object):\n",
    "\n",
    "    def __init__(self, image_size):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def __call__(self, data):\n",
    "        image, label = data\n",
    "        height, width = image.shape[:2]\n",
    "        image = cv2.resize(image, (self.image_size, self.image_size))\n",
    "        width_ratio = float(self.image_size) / width\n",
    "        height_ratio = float(self.image_size) / height\n",
    "        new_label = []\n",
    "        for lb in label:\n",
    "            resized_xmin = lb[0] * width_ratio\n",
    "            resized_ymin = lb[1] * height_ratio\n",
    "            resized_xmax = lb[2] * width_ratio\n",
    "            resized_ymax = lb[3] * height_ratio\n",
    "            resize_width = resized_xmax - resized_xmin\n",
    "            resize_height = resized_ymax - resized_ymin\n",
    "            new_label.append([resized_xmin, resized_ymin, resize_width, resize_height, lb[4]])\n",
    "\n",
    "        return image, new_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78db6343",
   "metadata": {},
   "source": [
    "## Yolo dataset"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 30,
>>>>>>> 7763e36cb6e79f4149f5793a02154d2b7caa5931
   "id": "00057c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class yoloDataset(data.Dataset):\n",
    "    image_size = 416 # Size to be aligned\n",
    "\n",
    "    # Parsing data list\n",
    "    def __init__(self,root,list_file,train):\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.fnames = []\n",
    "        self.objects = [] # [x_min, y_min, x_max, y_max, class]\n",
    "        self.classes = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow',\n",
    "                        'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train',\n",
    "                        'tvmonitor']\n",
    "        self.num_classes = len(self.classes)\n",
    "\n",
    "        with open(list_file) as f:\n",
    "            lines  = f.readlines()\n",
    "\n",
    "        # format of each line: filename (x_min, y_min, x_max, y_max, label) * object_num\n",
    "        for line in lines:\n",
    "            splited = line.strip().split() # .strip(): reomove space, tab from the end of each line\n",
    "            self.fnames.append(splited[0])\n",
    "            num_boxes = (len(splited) - 1) // 5\n",
    "            one_object = []\n",
    "            for i in range(num_boxes):\n",
    "                x_min = float(splited[1+5*i])\n",
    "                y_min = float(splited[2+5*i])\n",
    "                x_max = float(splited[3+5*i])\n",
    "                y_max = float(splited[4+5*i])\n",
    "                c = splited[5+5*i]\n",
    "                # int(c)+1\n",
    "                one_object.append([x_min,y_min,x_max,y_max, int(c)])\n",
    "            self.objects.append(one_object)\n",
    "        self.num_samples = len(self.objects)\n",
    "\n",
    "    # Getting single transformed, preprocessed image and its bounding boxes\n",
    "    def __getitem__(self,idx):\n",
    "        fname = self.fnames[idx]\n",
    "        img = cv2.imread(os.path.join(self.root+fname))\n",
    "        image_objects = self.objects[idx]\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.train:\n",
    "            transformations = Compose([HSVAdjust(), VerticalFlip(), Crop(), Resize(self.image_size)])\n",
    "        else:\n",
    "            transformations = Compose([Resize(self.image_size)])\n",
    "\n",
    "        # h,w,_ = img.shape\n",
    "        # boxes /= torch.Tensor([w,h,w,h]).expand_as(boxes)    -> replaced by batch-norm(?)\n",
    "        # img = self.subMean(img,self.mean)\n",
    "        # to see the whih grid is the box is located at\n",
    "        # .expand_as(other): expand this tensor as other\n",
    "        # [w, h, w, h] (1, 4) will be expanded to (#box, 4)\n",
    "        img, image_objects = transformations((img, image_objects))\n",
    "#         print(len(image_objects))\n",
    "\n",
    "        # img.shape (416,416,3) -> (3,416,416)\n",
    "        return np.transpose(np.array(img, dtype=np.float32), (2, 0, 1)), np.array(image_objects, dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1ffb82",
   "metadata": {},
   "source": [
    "## Yolo structure"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 31,
>>>>>>> 7763e36cb6e79f4149f5793a02154d2b7caa5931
   "id": "1f47d0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_layer(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False, isMaxpool=False):\n",
    "    if(isMaxpool):\n",
    "        return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding,bias=bias),\n",
    "                             nn.BatchNorm2d(out_channels),\n",
    "                             nn.LeakyReLU(0.1, inplace=True),\n",
    "                             nn.MaxPool2d(2,2))\n",
    "    else:\n",
    "        return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding,bias=bias),\n",
    "                             nn.BatchNorm2d(out_channels),\n",
    "                             nn.LeakyReLU(0.1, inplace=True))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 32,
>>>>>>> 7763e36cb6e79f4149f5793a02154d2b7caa5931
   "id": "caf5fc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Yolov2(nn.Module):\n",
    "    def __init__(self, num_classes, \n",
    "                 anchors = [(1.3221, 1.73145), (3.19275, 4.00944), (5.05587, 8.09892), (9.47112, 4.84053),(11.2364, 10.0071)]):\n",
    "        super(Yolov2,self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.anchors = anchors\n",
    "        # darnet19\n",
    "        self.stage1_c1 = build_layer(3, 32, 3, 1, 1, bias=False, isMaxpool=True)        \n",
    "        self.stage1_c2 = build_layer(32, 64, 3, 1, 1, bias=False, isMaxpool=True)\n",
    "        \n",
    "        self.stage1_c3 = build_layer(64, 128, 3, 1, 1, bias=False, isMaxpool=False)        \n",
    "        self.stage1_c4 = build_layer(128, 64, 1, 1, 0, bias=False, isMaxpool=False)        \n",
    "        self.stage1_c5 = build_layer(64, 128, 3, 1, 1, bias=False, isMaxpool=True)  \n",
    "        \n",
    "        self.stage1_c6 = build_layer(128, 256, 3, 1, 1, bias=False, isMaxpool=False)        \n",
    "        self.stage1_c7 = build_layer(256, 128, 1, 1, 0, bias=False, isMaxpool=False)        \n",
    "        self.stage1_c8 = build_layer(128, 256, 3, 1, 1, bias=False, isMaxpool=True) \n",
    "        \n",
    "        self.stage1_c9 = build_layer(256, 512, 3, 1, 1, bias=False, isMaxpool=False)        \n",
    "        self.stage1_c10 = build_layer(512, 256, 1, 1, 0, bias=False, isMaxpool=False)        \n",
    "        self.stage1_c11 = build_layer(256, 512, 3, 1, 1, bias=False, isMaxpool=False)        \n",
    "        self.stage1_c12 = build_layer(512, 256, 1, 1, 0, bias=False, isMaxpool=False)        \n",
    "        self.stage1_c13 = build_layer(256, 512, 3, 1, 1, bias=False, isMaxpool=False)\n",
    "        \n",
    "        self.stage2_maxpool = nn.MaxPool2d(2, 2)\n",
    "        self.stage2_c1 = build_layer(512, 1024, 3, 1, 1, bias=False, isMaxpool=False)        \n",
    "        self.stage2_c2 = build_layer(1024, 512, 1, 1, 0, bias=False, isMaxpool=False)        \n",
    "        self.stage2_c3 = build_layer(512, 1024, 3, 1, 1, bias=False, isMaxpool=False)        \n",
    "        self.stage2_c4 = build_layer(1024, 512, 1, 1, 0, bias=False, isMaxpool=False)        \n",
    "        self.stage2_c5 = build_layer(512, 1024, 3, 1, 1, bias=False, isMaxpool=False)\n",
    "        \n",
    "        self.stage2_c6 = build_layer(1024, 1024, 3, 1, 1, bias=False, isMaxpool=False)\n",
    "        self.stage2_c7 = build_layer(1024, 1024, 3, 1, 1, bias=False, isMaxpool=False)\n",
    "        \n",
    "        self.stage2_residual_c1 = build_layer(512, 64, 1, 1, 0, bias=False, isMaxpool=False)\n",
    "        \n",
    "        # classify head\n",
    "        self.stage3_c1 = build_layer(256+1024, 1024, 1, 1, 0, bias=False, isMaxpool=False)\n",
    "        self.stage3_c2 = nn.Conv2d(1024, len(self.anchors)*(5+self.num_classes), 1, 1, 0, bias=False)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output_1 = self.stage1_c1(input)\n",
    "        output_1 = self.stage1_c2(output_1)\n",
    "        output_1 = self.stage1_c3(output_1)\n",
    "        output_1 = self.stage1_c4(output_1)\n",
    "        output_1 = self.stage1_c5(output_1)\n",
    "        output_1 = self.stage1_c6(output_1)\n",
    "        output_1 = self.stage1_c7(output_1)\n",
    "        output_1 = self.stage1_c8(output_1)\n",
    "        output_1 = self.stage1_c9(output_1)\n",
    "        output_1 = self.stage1_c10(output_1)\n",
    "        output_1 = self.stage1_c11(output_1)\n",
    "        output_1 = self.stage1_c12(output_1)\n",
    "        output_1 = self.stage1_c13(output_1)\n",
    "\n",
    "        residual = output_1\n",
    "\n",
    "        output_2 = self.stage2_maxpool(output_1)\n",
    "        output_2 = self.stage2_c1(output_2)\n",
    "        output_2 = self.stage2_c2(output_2)\n",
    "        output_2 = self.stage2_c3(output_2)\n",
    "        output_2 = self.stage2_c4(output_2)\n",
    "        output_2 = self.stage2_c5(output_2)\n",
    "        output_2 = self.stage2_c6(output_2)\n",
    "        output_2 = self.stage2_c7(output_2)\n",
    "        \n",
    "        # rearrange\n",
    "        output_residual = self.stage2_residual_c1(residual)\n",
    "        batch_size, num_channel, height, width = output_residual.data.size()\n",
    "        output_residual = output_residual.view(batch_size, int(num_channel / 4), height, 2, width, 2).contiguous()\n",
    "        output_residual = output_residual.permute(0, 3, 5, 1, 2, 4).contiguous()\n",
    "        output_residual = output_residual.view(batch_size, -1, int(height / 2), int(width / 2))\n",
    "        \n",
    "        # concate residual block\n",
    "        output = torch.cat((output_2, output_residual), 1)\n",
    "        output = self.stage3_c1(output)\n",
    "        output = self.stage3_c2(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 33,
>>>>>>> 7763e36cb6e79f4149f5793a02154d2b7caa5931
   "id": "05e53bf8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchsummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m Yolov2(\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m416\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m416\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Lab_env/lib/python3.9/site-packages/torchsummary/torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39mapply(register_hook)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# make a forward pass\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# remove these hooks\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
      "File \u001b[0;32m~/anaconda3/envs/Lab_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mYolov2.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m---> 42\u001b[0m     output_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstage1_c1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     output_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage1_c2(output_1)\n\u001b[1;32m     44\u001b[0m     output_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage1_c3(output_1)\n",
      "File \u001b[0;32m~/anaconda3/envs/Lab_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/Lab_env/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/Lab_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1071\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1068\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks)\n\u001b[1;32m   1069\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m-> 1071\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1073\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m   1074\u001b[0m             _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   1075\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n",
      "File \u001b[0;32m~/anaconda3/envs/Lab_env/lib/python3.9/site-packages/torch/nn/modules/conv.py:443\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 443\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Lab_env/lib/python3.9/site-packages/torch/nn/modules/conv.py:439\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    437\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    438\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "model = Yolov2(20)\n",
    "summary(model, (3,416,416))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49ab0ad",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
=======
   "execution_count": 34,
>>>>>>> 7763e36cb6e79f4149f5793a02154d2b7caa5931
   "id": "b007e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLoss(nn.modules.loss._Loss):\n",
    "    # The loss I borrow from LightNet repo.\n",
    "    def __init__(self, num_classes, anchors, reduction=32, coord_scale=1.0, noobject_scale=1.0,\n",
    "                 object_scale=5.0, class_scale=1.0, thresh=0.6):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_anchors = len(anchors)\n",
    "        self.anchor_step = len(anchors[0])\n",
    "        self.anchors = torch.Tensor(anchors)\n",
    "        self.reduction = reduction\n",
    "\n",
    "        self.coord_scale = coord_scale\n",
    "        self.noobject_scale = noobject_scale\n",
    "        self.object_scale = object_scale\n",
    "        self.class_scale = class_scale\n",
    "        self.thresh = thresh\n",
    "\n",
    "    def forward(self, output, target):\n",
    "\n",
    "        batch_size = output.data.size(0)\n",
    "        height = output.data.size(2)\n",
    "        width = output.data.size(3)\n",
    "\n",
    "        # Get x,y,w,h,conf,cls\n",
    "        output = output.view(batch_size, self.num_anchors, -1, height * width)\n",
    "        coord = torch.zeros_like(output[:, :, :4, :])\n",
    "        coord[:, :, :2, :] = output[:, :, :2, :].sigmoid()  \n",
    "        coord[:, :, 2:4, :] = output[:, :, 2:4, :]\n",
    "        conf = output[:, :, 4, :].sigmoid()\n",
    "        cls = output[:, :, 5:, :].contiguous().view(batch_size * self.num_anchors, self.num_classes,\n",
    "                                                    height * width).transpose(1, 2).contiguous().view(-1,\n",
    "                                                                                                      self.num_classes)\n",
    "\n",
    "        # Create prediction boxes\n",
    "        pred_boxes = torch.FloatTensor(batch_size * self.num_anchors * height * width, 4)\n",
    "        lin_x = torch.range(0, width - 1).repeat(height, 1).view(height * width)\n",
    "        lin_y = torch.range(0, height - 1).repeat(width, 1).t().contiguous().view(height * width)\n",
    "        anchor_w = self.anchors[:, 0].contiguous().view(self.num_anchors, 1)\n",
    "        anchor_h = self.anchors[:, 1].contiguous().view(self.num_anchors, 1)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            pred_boxes = pred_boxes.cuda()\n",
    "            lin_x = lin_x.cuda()\n",
    "            lin_y = lin_y.cuda()\n",
    "            anchor_w = anchor_w.cuda()\n",
    "            anchor_h = anchor_h.cuda()\n",
    "\n",
    "        pred_boxes[:, 0] = (coord[:, :, 0].detach() + lin_x).view(-1)\n",
    "        pred_boxes[:, 1] = (coord[:, :, 1].detach() + lin_y).view(-1)\n",
    "        pred_boxes[:, 2] = (coord[:, :, 2].detach().exp() * anchor_w).view(-1)\n",
    "        pred_boxes[:, 3] = (coord[:, :, 3].detach().exp() * anchor_h).view(-1)\n",
    "        pred_boxes = pred_boxes.cpu()\n",
    "\n",
    "        # Get target values\n",
    "        coord_mask, conf_mask, cls_mask, tcoord, tconf, tcls = self.build_targets(pred_boxes, target, height, width)\n",
    "        coord_mask = coord_mask.expand_as(tcoord)\n",
    "        tcls = tcls[cls_mask].view(-1).long()\n",
    "        cls_mask = cls_mask.view(-1, 1).repeat(1, self.num_classes)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            tcoord = tcoord.cuda()\n",
    "            tconf = tconf.cuda()\n",
    "            coord_mask = coord_mask.cuda()\n",
    "            conf_mask = conf_mask.cuda()\n",
    "            tcls = tcls.cuda()\n",
    "            cls_mask = cls_mask.cuda()\n",
    "\n",
    "        conf_mask = conf_mask.sqrt()\n",
    "        cls = cls[cls_mask].view(-1, self.num_classes)\n",
    "\n",
    "        # Compute losses\n",
    "        mse = nn.MSELoss(size_average=False)\n",
    "        ce = nn.CrossEntropyLoss(size_average=False)\n",
    "        self.loss_coord = self.coord_scale * mse(coord * coord_mask, tcoord * coord_mask) / batch_size\n",
    "        self.loss_conf = mse(conf * conf_mask, tconf * conf_mask) / batch_size\n",
    "        self.loss_cls = self.class_scale * 2 * ce(cls, tcls) / batch_size\n",
    "        self.loss_tot = self.loss_coord + self.loss_conf + self.loss_cls\n",
    "\n",
    "        return self.loss_tot, self.loss_coord, self.loss_conf, self.loss_cls\n",
    "\n",
    "    def build_targets(self, pred_boxes, ground_truth, height, width):\n",
    "        batch_size = len(ground_truth)\n",
    "\n",
    "        conf_mask = torch.ones(batch_size, self.num_anchors, height * width, requires_grad=False) * self.noobject_scale\n",
    "        coord_mask = torch.zeros(batch_size, self.num_anchors, 1, height * width, requires_grad=False)\n",
    "        cls_mask = torch.zeros(batch_size, self.num_anchors, height * width, requires_grad=False).byte()\n",
    "        tcoord = torch.zeros(batch_size, self.num_anchors, 4, height * width, requires_grad=False)\n",
    "        tconf = torch.zeros(batch_size, self.num_anchors, height * width, requires_grad=False)\n",
    "        tcls = torch.zeros(batch_size, self.num_anchors, height * width, requires_grad=False)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            if len(ground_truth[b]) == 0:\n",
    "                continue\n",
    "\n",
    "            # Build up tensors\n",
    "            cur_pred_boxes = pred_boxes[\n",
    "                             b * (self.num_anchors * height * width):(b + 1) * (self.num_anchors * height * width)]\n",
    "            if self.anchor_step == 4:\n",
    "                anchors = self.anchors.clone()\n",
    "                anchors[:, :2] = 0\n",
    "            else:\n",
    "                anchors = torch.cat([torch.zeros_like(self.anchors), self.anchors], 1)\n",
    "            gt = torch.zeros(len(ground_truth[b]), 4)\n",
    "            for i, anno in enumerate(ground_truth[b]):\n",
    "                gt[i, 0] = (anno[0] + anno[2] / 2) / self.reduction\n",
    "                gt[i, 1] = (anno[1] + anno[3] / 2) / self.reduction\n",
    "                gt[i, 2] = anno[2] / self.reduction\n",
    "                gt[i, 3] = anno[3] / self.reduction\n",
    "\n",
    "            # Set confidence mask of matching detections to 0\n",
    "            iou_gt_pred = bbox_ious(gt, cur_pred_boxes)\n",
    "            mask = (iou_gt_pred > self.thresh).sum(0) >= 1\n",
    "            conf_mask[b][mask.view_as(conf_mask[b])] = 0\n",
    "\n",
    "            # Find best anchor for each ground truth\n",
    "            gt_wh = gt.clone()\n",
    "            gt_wh[:, :2] = 0\n",
    "            iou_gt_anchors = bbox_ious(gt_wh, anchors)\n",
    "            _, best_anchors = iou_gt_anchors.max(1)\n",
    "\n",
    "            # Set masks and target values for each ground truth\n",
    "            for i, anno in enumerate(ground_truth[b]):\n",
    "                gi = min(width - 1, max(0, int(gt[i, 0])))\n",
    "                gj = min(height - 1, max(0, int(gt[i, 1])))\n",
    "                best_n = best_anchors[i]\n",
    "                iou = iou_gt_pred[i][best_n * height * width + gj * width + gi]\n",
    "                coord_mask[b][best_n][0][gj * width + gi] = 1\n",
    "                cls_mask[b][best_n][gj * width + gi] = 1\n",
    "                conf_mask[b][best_n][gj * width + gi] = self.object_scale\n",
    "                tcoord[b][best_n][0][gj * width + gi] = gt[i, 0] - gi\n",
    "                tcoord[b][best_n][1][gj * width + gi] = gt[i, 1] - gj\n",
    "                tcoord[b][best_n][2][gj * width + gi] = math.log(max(gt[i, 2], 1.0) / self.anchors[best_n, 0])\n",
    "                tcoord[b][best_n][3][gj * width + gi] = math.log(max(gt[i, 3], 1.0) / self.anchors[best_n, 1])\n",
    "                tconf[b][best_n][gj * width + gi] = iou\n",
    "                tcls[b][best_n][gj * width + gi] = int(anno[4])\n",
    "\n",
    "        return coord_mask, conf_mask, cls_mask, tcoord, tconf, tcls\n",
    "\n",
    "\n",
    "def bbox_ious(boxes1, boxes2):\n",
    "    b1x1, b1y1 = (boxes1[:, :2] - (boxes1[:, 2:4] / 2)).split(1, 1)\n",
    "    b1x2, b1y2 = (boxes1[:, :2] + (boxes1[:, 2:4] / 2)).split(1, 1)\n",
    "    b2x1, b2y1 = (boxes2[:, :2] - (boxes2[:, 2:4] / 2)).split(1, 1)\n",
    "    b2x2, b2y2 = (boxes2[:, :2] + (boxes2[:, 2:4] / 2)).split(1, 1)\n",
    "\n",
    "    dx = (b1x2.min(b2x2.t()) - b1x1.max(b2x1.t())).clamp(min=0)\n",
    "    dy = (b1y2.min(b2y2.t()) - b1y1.max(b2y1.t())).clamp(min=0)\n",
    "    intersections = dx * dy\n",
    "\n",
    "    areas1 = (b1x2 - b1x1) * (b1y2 - b1y1)\n",
    "    areas2 = (b2x2 - b2x1) * (b2y2 - b2y1)\n",
    "    unions = (areas1 + areas2.t()) - intersections\n",
    "\n",
    "    return intersections / unions"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
=======
   "execution_count": 35,
>>>>>>> 7763e36cb6e79f4149f5793a02154d2b7caa5931
   "id": "41d68225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    items = list(zip(*batch))\n",
    "    items[0] = default_collate(items[0])\n",
    "    items[1] = list(items[1])\n",
    "    return items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e893d3",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
=======
   "execution_count": 36,
>>>>>>> 7763e36cb6e79f4149f5793a02154d2b7caa5931
   "id": "b1cd74c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "root = 'VOCdevkit/VOC2007/JPEGImages/'\n",
    "reduction = 32\n",
    "momentum = 0.9\n",
    "decay = 0.0005\n",
    "valid_interval = 10\n",
    "epoches = 50\n",
    "epoch_patience = 20\n",
    "pre_trained_model_path = ''\n",
    "\n",
    "def train():\n",
    "    # check cuda\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(123)\n",
    "    else:\n",
    "        torch.manual_seed(123)\n",
    "        \n",
    "    # set scheduler\n",
    "    learning_rate_schedule = {\"0\": 1e-5, \"5\": 1e-4, \"80\": 1e-5, \"110\": 1e-6}\n",
    "    \n",
    "    # read data\n",
    "    train_params = {\"batch_size\": batch_size,\n",
    "                       \"shuffle\": True,\n",
    "                       \"drop_last\": True,\n",
    "                   \"collate_fn\": custom_collate_fn}\n",
    "\n",
    "    valid_params = {\"batch_size\": batch_size,\n",
    "                   \"shuffle\": False,\n",
    "                   \"drop_last\": False,\n",
    "                   \"collate_fn\": custom_collate_fn}\n",
    "    \n",
    "    train_set = yoloDataset(root, root+'voc2007train.txt',train=True)\n",
    "    train_generator = DataLoader(train_set, **train_params)\n",
    "\n",
    "    valid_set = yoloDataset(root, root+'voc2007valid.txt',train=False)\n",
    "    valid_generator = DataLoader(valid_set, **valid_params)\n",
    "    \n",
    "    # load pretrain weight\n",
    "    if torch.cuda.is_available():\n",
    "        model = Yolov2(train_set.num_classes)\n",
    "#         model.load_state_dict(torch.load(pre_trained_model_path))\n",
    "    else:\n",
    "        model = Yolov2(train_set.num_classes)\n",
    "#         model.load_state_dict(torch.load(pre_trained_model_path, map_location=lambda storage, loc: storage))\n",
    "    # The following line will re-initialize weight for the last layer, which is useful\n",
    "    # when you want to retrain the model based on my trained weights. if you uncomment it,\n",
    "    # you will see the loss is already very small at the beginning.\n",
    "#     nn.init.normal_(list(model.modules())[-1].weight, 0, 0.01)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    criterion = YoloLoss(train_set.num_classes, model.anchors, reduction)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-5, momentum=momentum, weight_decay=decay)\n",
    "    best_loss = 1e10\n",
    "    best_epoch = 0\n",
    "    \n",
    "    # start train and valid\n",
    "    model.train()\n",
    "    num_iter_per_epoch = len(train_generator)\n",
    "    for epoch in range(epoches):\n",
    "        # scheduler change learning rate\n",
    "        if str(epoch) in learning_rate_schedule.keys():\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = learning_rate_schedule[str(epoch)]\n",
    "                \n",
    "        # batch train\n",
    "        for iter, batch in enumerate(train_generator):\n",
    "            image, label = batch\n",
    "            if torch.cuda.is_available():\n",
    "                image = Variable(image.cuda(), requires_grad=True)\n",
    "            else:\n",
    "                image = Variable(image, requires_grad=True)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(image)\n",
    "            loss, loss_coord, loss_conf, loss_cls = criterion(logits, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(\"Epoch: {}/{}, Iteration: {}/{}, Lr: {}, Loss:{:.2f} (Coord:{:.2f} Conf:{:.2f} Cls:{:.2f})\".format(\n",
    "                epoch + 1,\n",
    "                epoches,\n",
    "                iter + 1,\n",
    "                num_iter_per_epoch,\n",
    "                optimizer.param_groups[0]['lr'],\n",
    "                loss,\n",
    "                loss_coord,\n",
    "                loss_conf,\n",
    "                loss_cls))\n",
    "        \n",
    "        # valid\n",
    "        if epoch % valid_interval == 0:\n",
    "            model.eval()\n",
    "            loss_ls = []\n",
    "            loss_coord_ls = []\n",
    "            loss_conf_ls = []\n",
    "            loss_cls_ls = []\n",
    "            for valid_iter, valid_batch in enumerate(valid_generator):\n",
    "                valid_image, valid_label = valid_batch\n",
    "                num_sample = len(valid_label)\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    valid_image = valid_image.cuda()\n",
    "                    \n",
    "                with torch.no_grad():\n",
    "                    valid_logits = model(valid_image)\n",
    "                    batch_loss, batch_loss_coord, batch_loss_conf, batch_loss_cls = criterion(valid_logits, valid_label)\n",
    "                # record loss\n",
    "                loss_ls.append(batch_loss * num_sample)\n",
    "                loss_coord_ls.append(batch_loss_coord * num_sample)\n",
    "                loss_conf_ls.append(batch_loss_conf * num_sample)\n",
    "                loss_cls_ls.append(batch_loss_cls * num_sample)\n",
    "                \n",
    "            valid_loss = sum(loss_ls) / valid_set.__len__()\n",
    "            valid_coord_loss = sum(loss_coord_ls) / valid_set.__len__()\n",
    "            valid_conf_loss = sum(loss_conf_ls) / valid_set.__len__()\n",
    "            valid_cls_loss = sum(loss_cls_ls) / valid_set.__len__()\n",
    "            print(\"Epoch: {}/{}, Lr: {}, Loss:{:.2f} (Coord:{:.2f} Conf:{:.2f} Cls:{:.2f})\".format(\n",
    "                epoch + 1,\n",
    "                epoches,\n",
    "                optimizer.param_groups[0]['lr'],\n",
    "                valid_loss,\n",
    "                valid_coord_loss,\n",
    "                valid_conf_loss,\n",
    "                valid_cls_loss))\n",
    "            \n",
    "            # change back to train mode\n",
    "            model.train()\n",
    "            \n",
    "            # save the best model weight\n",
    "            if te_loss + opt.es_min_delta < best_loss:\n",
    "                best_loss = te_loss\n",
    "                best_epoch = epoch\n",
    "                torch.save(model.state_dict(), os.path.join('model_best.pth'))\n",
    "\n",
    "            # Early stopping\n",
    "            if epoch - best_epoch > epoch_patience > 0:\n",
    "                print(\"Stop training at epoch {}. The lowest loss achieved is {}\".format(epoch, te_loss))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 20,
=======
   "execution_count": 37,
>>>>>>> 7763e36cb6e79f4149f5793a02154d2b7caa5931
   "id": "9b2c05b0",
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m():\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# check cuda\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m---> 14\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m123\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m         torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m123\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/Lab_env/lib/python3.9/site-packages/torch/cuda/random.py:95\u001b[0m, in \u001b[0;36mmanual_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     92\u001b[0m     default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[idx]\n\u001b[1;32m     93\u001b[0m     default_generator\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[0;32m---> 95\u001b[0m \u001b[43m_lazy_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Lab_env/lib/python3.9/site-packages/torch/cuda/__init__.py:116\u001b[0m, in \u001b[0;36m_lazy_call\u001b[0;34m(callable)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_lazy_call\u001b[39m(callable):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 116\u001b[0m         \u001b[43mcallable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;66;03m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[39;00m\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;66;03m# file system to get traceback info. Patch linecache or do something\u001b[39;00m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;66;03m# else here if this ends up being important.\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \n\u001b[1;32m    122\u001b[0m         \u001b[38;5;66;03m# Don't store the actual traceback to avoid memory cycle\u001b[39;00m\n\u001b[1;32m    123\u001b[0m         _queued_calls\u001b[38;5;241m.\u001b[39mappend((callable, traceback\u001b[38;5;241m.\u001b[39mformat_stack()))\n",
      "File \u001b[0;32m~/anaconda3/envs/Lab_env/lib/python3.9/site-packages/torch/cuda/random.py:93\u001b[0m, in \u001b[0;36mmanual_seed.<locals>.cb\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m idx \u001b[38;5;241m=\u001b[39m current_device()\n\u001b[1;32m     92\u001b[0m default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[idx]\n\u001b[0;32m---> 93\u001b[0m \u001b[43mdefault_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
=======
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zk/qnl0l6n528q5_y9qf1nbc0mm0000gn/T/ipykernel_21470/2922291331.py:36: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  lin_x = torch.range(0, width - 1).repeat(height, 1).view(height * width)\n",
      "/var/folders/zk/qnl0l6n528q5_y9qf1nbc0mm0000gn/T/ipykernel_21470/2922291331.py:37: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  lin_y = torch.range(0, height - 1).repeat(width, 1).t().contiguous().view(height * width)\n",
      "/var/folders/zk/qnl0l6n528q5_y9qf1nbc0mm0000gn/T/ipykernel_21470/2922291331.py:57: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /Users/distiller/project/pytorch/aten/src/ATen/native/IndexingUtils.h:28.)\n",
      "  tcls = tcls[cls_mask].view(-1).long()\n",
      "/var/folders/zk/qnl0l6n528q5_y9qf1nbc0mm0000gn/T/ipykernel_21470/2922291331.py:69: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /Users/distiller/project/pytorch/aten/src/ATen/native/IndexingUtils.h:28.)\n",
      "  cls = cls[cls_mask].view(-1, self.num_classes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50, Iteration: 1/438, Lr: 1e-05, Loss:218.27 (Coord:2.36 Conf:205.85 Cls:10.07)\n",
      "Epoch: 1/50, Iteration: 2/438, Lr: 1e-05, Loss:210.01 (Coord:1.78 Conf:197.70 Cls:10.52)\n",
      "Epoch: 1/50, Iteration: 3/438, Lr: 1e-05, Loss:188.53 (Coord:0.49 Conf:180.62 Cls:7.43)\n",
      "Epoch: 1/50, Iteration: 4/438, Lr: 1e-05, Loss:182.03 (Coord:3.01 Conf:162.64 Cls:16.38)\n",
      "Epoch: 1/50, Iteration: 5/438, Lr: 1e-05, Loss:158.47 (Coord:2.65 Conf:137.06 Cls:18.76)\n",
      "Epoch: 1/50, Iteration: 6/438, Lr: 1e-05, Loss:128.96 (Coord:1.14 Conf:117.66 Cls:10.17)\n",
      "Epoch: 1/50, Iteration: 7/438, Lr: 1e-05, Loss:107.39 (Coord:2.50 Conf:90.56 Cls:14.32)\n",
      "Epoch: 1/50, Iteration: 8/438, Lr: 1e-05, Loss:98.81 (Coord:4.37 Conf:73.34 Cls:21.10)\n",
      "Epoch: 1/50, Iteration: 9/438, Lr: 1e-05, Loss:78.86 (Coord:3.82 Conf:54.85 Cls:20.19)\n",
      "Epoch: 1/50, Iteration: 10/438, Lr: 1e-05, Loss:61.84 (Coord:1.94 Conf:43.76 Cls:16.15)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(image)\n\u001b[1;32m     76\u001b[0m loss, loss_coord, loss_conf, loss_cls \u001b[38;5;241m=\u001b[39m criterion(logits, label)\n\u001b[0;32m---> 77\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, Iteration: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, Lr: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, Loss:\u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m (Coord:\u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m Conf:\u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m Cls:\u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     80\u001b[0m     epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     81\u001b[0m     epoches,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m     loss_conf,\n\u001b[1;32m     88\u001b[0m     loss_cls))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
>>>>>>> 7763e36cb6e79f4149f5793a02154d2b7caa5931
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358213f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
