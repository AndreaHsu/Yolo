{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKvr9DC6x9QM"
   },
   "source": [
    "# Object Detection\n",
    "* 2 people in a group\n",
    "* Deadline: 10/13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zPl8L7qx9QP"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "- PASCAL VOC 2007\n",
    "  - Number of class: 20\n",
    "  - The data list is provided in the google drive. However, you have to download the training/testing data from http://host.robots.ox.ac.uk/pascal/VOC/voc2007/. \n",
    "    - Train/Val data: 5011\n",
    "        - Each row contains one image and its bounding boxes.\n",
    "        - filename ($x_{min}$, $y_{min}$, $x_{max}$, $y_{max}$, $label$) $\\times$ object_num\n",
    "        - class idx starts from 1\n",
    "    - Test data: 4951\n",
    "        - filename ($x_{min}$, $y_{min}$, $x_{max}$, $y_{max}$, $label$) $\\times$ object_num\n",
    "        - class idx starts from 0\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXto_Mqqx9QQ"
   },
   "source": [
    "### Loading your data into Google Colab with Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20080,
     "status": "ok",
     "timestamp": 1664613762229,
     "user": {
      "displayName": "夏宇澄",
      "userId": "12299807091212530744"
     },
     "user_tz": -480
    },
    "id": "uZ4-AYFzx9QQ",
    "outputId": "303d704d-b754-426b-8e91-5a7e34f1b342"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knUoH4P4x9QR"
   },
   "source": [
    "## Resnet50 backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 285,
     "status": "ok",
     "timestamp": 1664613766309,
     "user": {
      "displayName": "夏宇澄",
      "userId": "12299807091212530744"
     },
     "user_tz": -480
    },
    "id": "ykAGukXsx9QS"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.resnet import BasicBlock, Bottleneck\n",
    "from torchvision.models.resnet import model_urls\n",
    "from torchsummary import summary\n",
    "\n",
    "class classify_bottleneck(nn.Module):\n",
    "  expansion = 1\n",
    "\n",
    "  def __init__(self, inplanes, planes, stride=1, block_type='A'):\n",
    "    super(classify_bottleneck, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "    self.bn1 = nn.BatchNorm2d(planes)\n",
    "    self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=2, bias=False,dilation=2)\n",
    "    self.bn2 = nn.BatchNorm2d(planes)\n",
    "    self.conv3 = nn.Conv2d(planes, planes, kernel_size=1, bias=False)\n",
    "    self.bn3 = nn.BatchNorm2d(planes)\n",
    "\n",
    "    self.downsample = nn.Sequential()\n",
    "    if stride != 1 or block_type=='B':\n",
    "        self.downsample = nn.Sequential(\n",
    "            nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "            nn.BatchNorm2d(self.expansion*planes)\n",
    "        )\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = F.relu(self.bn1(self.conv1(x)))\n",
    "    out = F.relu(self.bn2(self.conv2(out)))\n",
    "    out = self.bn3(self.conv3(out))\n",
    "    out += self.downsample(x)\n",
    "    out = F.relu(out)\n",
    "    return out\n",
    "\n",
    "class ResNetYoloV1(nn.Module):\n",
    "\n",
    "    def __init__(self, resnet_type):\n",
    "\t\n",
    "        resnet_spec = {18: (BasicBlock, [2, 2, 2, 2], [64, 64, 128, 256, 512], 'resnet18'),\n",
    "\t\t       34: (BasicBlock, [3, 4, 6, 3], [64, 64, 128, 256, 512], 'resnet34'),\n",
    "\t\t       50: (Bottleneck, [3, 4, 6, 3], [64, 256, 512, 1024, 2048], 'resnet50'),\n",
    "\t\t       101: (Bottleneck, [3, 4, 23, 3], [64, 256, 512, 1024, 2048], 'resnet101'),\n",
    "\t\t       152: (Bottleneck, [3, 8, 36, 3], [64, 256, 512, 1024, 2048], 'resnet152')}\n",
    "        block, layers, channels, name = resnet_spec[resnet_type]\n",
    "        \n",
    "        self.name = name\n",
    "        self.inplanes = 64\n",
    "        super(ResNetYoloV1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        self.layer5 = self._make_classify_layer(in_channels=2048) #2048*14*14\n",
    "\n",
    "        self.conv_end = nn.Conv2d(256, 30, kernel_size=3, stride=2, padding=1, bias=False)#30*7*7\n",
    "        self.bn_end = nn.BatchNorm2d(30)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.001)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _make_classify_layer(self,in_channels):\n",
    "        layers = []\n",
    "        layers.append(classify_bottleneck(inplanes=in_channels, planes=256, block_type='B'))\n",
    "        layers.append(classify_bottleneck(inplanes=256, planes=256))\n",
    "        layers.append(classify_bottleneck(inplanes=256, planes=256))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.layer2(x1)\n",
    "        x3 = self.layer3(x2)\n",
    "        x4 = self.layer4(x3)\n",
    "        # x4 layer output size: (B, 2048, 7, 7)\n",
    "        x5 = self.layer5(x4)\n",
    "        x = self.conv_end(x5)\n",
    "        x = self.bn_end(x)\n",
    "        x = torch.sigmoid(x) #归一化到0-1\n",
    "        # x = x.view(-1,7,7,30)\n",
    "        x = x.permute(0,2,3,1) #(-1,7,7,30)\n",
    "        return x\n",
    "\n",
    "    def init_weights(self):\n",
    "        org_resnet = torch.utils.model_zoo.load_url(model_urls[self.name])\n",
    "        # drop orginal resnet fc layer, add 'None' in case of no fc layer, that will raise error\n",
    "        org_resnet.pop('fc.weight', None)\n",
    "        org_resnet.pop('fc.bias', None)\n",
    "\n",
    "        self.load_state_dict(org_resnet)\n",
    "        print(\"Initialize resnet from model zoo\")\n",
    "\n",
    "def load_change_weights(model, model_name):\n",
    "  \n",
    "  org_resnet = torch.utils.model_zoo.load_url(model_urls[model_name])\n",
    "  org_resnet.pop('fc.weight', None)\n",
    "  org_resnet.pop('fc.bias', None)\n",
    "\n",
    "  dd = model.state_dict()\n",
    "  for k in org_resnet.keys():\n",
    "      # print(k)\n",
    "      if k in dd.keys() and not k.startswith('fc'):\n",
    "          # print('yes')\n",
    "          dd[k] = org_resnet[k]\n",
    "  model.load_state_dict(dd)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kj7b4Idx9QT"
   },
   "source": [
    "### Assignment\n",
    "You are required to build a model to perform object detection on the provided Pascal VOC dataset in this project.\n",
    "Here are some hints that help you to accomplish the project successfully.\n",
    "\n",
    "### Hints\n",
    "- YOLOv1 is the simplest and suggested model to be implemented.\n",
    "- Be careful of the normalization techniques on bounding boxes.\n",
    "    1. normalize the height and width with image size to fall into 0 and 1\n",
    "    2. x and y coordinates are parameterized to be the offsets of a particular grid cell and also bounded by 0 and 1\n",
    "- Loss function has a great impact on training stability.\n",
    "    1. loss function is the most important in this project, especially in calculating IOU\n",
    "    2. only one bounding box predictor is responsible for each object\n",
    "    3. weights for different types of losses\n",
    "    4. predict the square root of height and width instead of predicting them directly\n",
    "- Data augmentation.\n",
    "    1. It contains only 5011 images in total. Furthermore, the labels are highly imbalanced.\n",
    "    2. Random scaling and translations are applied when training YOLO.\n",
    "    3. Note that the bounding box coordinates have to be changed accordingly if the image was transformed.\n",
    "\n",
    "### Evaluation Metric\n",
    "- Please evaluate your model on Pascal VOC testing set using Mean Average Precision (mAP).\n",
    "- Write a brief report including your implementation, performance and  qualitative results(visualize bounding box on some images). \n",
    "- For more detailed explanation of mAP, please follow https://github.com/rafaelpadilla/Object-Detection-Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1664613312162,
     "user": {
      "displayName": "夏宇澄",
      "userId": "12299807091212530744"
     },
     "user_tz": -480
    },
    "id": "57fbGJtQdhTA"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MRA4OTgoeYy"
   },
   "source": [
    "## Dataset & data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 351,
     "status": "ok",
     "timestamp": 1664613768703,
     "user": {
      "displayName": "夏宇澄",
      "userId": "12299807091212530744"
     },
     "user_tz": -480
    },
    "id": "-unQlvZTocGb"
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class yoloDataset(data.Dataset):\n",
    "    image_size = 448 # Size to be aligned\n",
    "\n",
    "    # Parsing data list\n",
    "    def __init__(self,root,list_file,train,transform):\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.fnames = []\n",
    "        self.boxes = []\n",
    "        self.labels = []\n",
    "        self.mean = (123,117,104) # RGB\n",
    "\n",
    "        # Cat multiple list files together.\n",
    "        '''if isinstance(list_file, list):\n",
    "            # This is especially useful for voc07/voc12 combination.\n",
    "            tmp_file = '/tmp/listfile.txt'\n",
    "            os.system('cat %s > %s' % (' '.join(list_file), tmp_file))\n",
    "            list_file = tmp_file'''\n",
    "\n",
    "        with open(list_file) as f:\n",
    "            lines  = f.readlines()\n",
    "\n",
    "        # format of each line: filename (x_min, y_min, x_max, y_max, label) * object_num\n",
    "        for line in lines:\n",
    "            splited = line.strip().split() # .strip(): reomove space, tab from the end of each line\n",
    "            self.fnames.append(splited[0])\n",
    "            num_boxes = (len(splited) - 1) // 5\n",
    "            box=[]\n",
    "            label=[]\n",
    "            for i in range(num_boxes):\n",
    "                x = float(splited[1+5*i])\n",
    "                y = float(splited[2+5*i])\n",
    "                x2 = float(splited[3+5*i])\n",
    "                y2 = float(splited[4+5*i])\n",
    "                c = splited[5+5*i]\n",
    "                box.append([x,y,x2,y2])\n",
    "                label.append(int(c)+1) # +1: since the idx start from 0\n",
    "            self.boxes.append(torch.Tensor(box))\n",
    "            self.labels.append(torch.LongTensor(label))\n",
    "        self.num_samples = len(self.boxes)\n",
    "\n",
    "    # Getting single transformed, preprocessed image and its target\n",
    "    def __getitem__(self,idx):\n",
    "        fname = self.fnames[idx]\n",
    "        img = cv2.imread(os.path.join(self.root+fname))\n",
    "        boxes = self.boxes[idx].clone()\n",
    "        labels = self.labels[idx].clone()\n",
    "\n",
    "        # Randomly transforming image\n",
    "        if self.train:\n",
    "            #img = self.random_bright(img)\n",
    "            img, boxes = self.random_flip(img, boxes)\n",
    "            img,boxes = self.randomScale(img,boxes)\n",
    "            img = self.randomBlur(img)\n",
    "            img = self.RandomBrightness(img)\n",
    "            img = self.RandomHue(img)\n",
    "            img = self.RandomSaturation(img)\n",
    "            img,boxes,labels = self.randomShift(img,boxes,labels)\n",
    "            img,boxes,labels = self.randomCrop(img,boxes,labels)\n",
    "\n",
    "        # #debug: showing the transformed image\n",
    "        # box_show = boxes.numpy().reshape(-1)\n",
    "        # # print(box_show)\n",
    "        # img_show = self.BGR2RGB(img)\n",
    "        # pt1=(int(box_show[0]),int(box_show[1])); pt2=(int(box_show[2]),int(box_show[3]))\n",
    "        # cv2.rectangle(img_show,pt1=pt1,pt2=pt2,color=(0,255,0),thickness=1)\n",
    "        # plt.figure()\n",
    "        \n",
    "        # plt.imshow(img_show)\n",
    "        # plt.show()\n",
    "        # #debug\n",
    "\n",
    "        h,w,_ = img.shape\n",
    "        boxes /= torch.Tensor([w,h,w,h]).expand_as(boxes) \n",
    "        # .expand_as(other): expand this tensor as other\n",
    "        # [w, h, w, h] (1, 4) will be expanded to (#box, 4)\n",
    "\n",
    "        img = self.BGR2RGB(img) # because pytorch pretrained model use RGB\n",
    "        img = self.subMean(img,self.mean)\n",
    "        img = cv2.resize(img,(self.image_size,self.image_size))\n",
    "        target = self.encoder(boxes,labels) # 7x7x30, where 30 = 5*2(xywh+confidence for 2 boxes) + 20(classes)\n",
    "        for t in self.transform:\n",
    "            img = t(img)\n",
    "\n",
    "        return img,target\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    # Utils\n",
    "    # Encoding the boxes, labels for single image\n",
    "    def encoder(self,boxes,labels):\n",
    "        grid_num = 7\n",
    "        target = torch.zeros((grid_num,grid_num,30))\n",
    "        cell_size = 1./grid_num\n",
    "        wh = boxes[:,2:]-boxes[:,:2]\n",
    "        cxcy = (boxes[:,2:]+boxes[:,:2])/2\n",
    "        for i in range(cxcy.size()[0]):\n",
    "            cxcy_sample = cxcy[i]\n",
    "            ij = (cxcy_sample/cell_size).ceil()-1 #\n",
    "            target[int(ij[1]),int(ij[0]),4] = 1\n",
    "            target[int(ij[1]),int(ij[0]),9] = 1\n",
    "            target[int(ij[1]),int(ij[0]),int(labels[i])+9] = 1\n",
    "            xy = ij*cell_size # upper left coordinates of corresponding grid\n",
    "            delta_xy = (cxcy_sample -xy)/cell_size\n",
    "            target[int(ij[1]),int(ij[0]),2:4] = wh[i]\n",
    "            target[int(ij[1]),int(ij[0]),:2] = delta_xy\n",
    "            target[int(ij[1]),int(ij[0]),7:9] = wh[i]\n",
    "            target[int(ij[1]),int(ij[0]),5:7] = delta_xy\n",
    "        return target\n",
    "\n",
    "    def BGR2RGB(self,img):\n",
    "        return cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    def BGR2HSV(self,img):\n",
    "        return cv2.cvtColor(img,cv2.COLOR_BGR2HSV)\n",
    "    def HSV2BGR(self,img):\n",
    "        return cv2.cvtColor(img,cv2.COLOR_HSV2BGR)\n",
    "    \n",
    "    def subMean(self,bgr,mean):\n",
    "        mean = np.array(mean, dtype=np.float32)\n",
    "        bgr = bgr - mean\n",
    "        return bgr\n",
    "    \n",
    "    def RandomBrightness(self,bgr):\n",
    "        if random.random() < 0.5:\n",
    "            hsv = self.BGR2HSV(bgr)\n",
    "            h,s,v = cv2.split(hsv)\n",
    "            adjust = random.choice([0.5,1.5])\n",
    "            v = v*adjust\n",
    "            v = np.clip(v, 0, 255).astype(hsv.dtype)\n",
    "            hsv = cv2.merge((h,s,v))\n",
    "            bgr = self.HSV2BGR(hsv)\n",
    "        return bgr\n",
    "\n",
    "    def RandomSaturation(self,bgr):\n",
    "        if random.random() < 0.5:\n",
    "            hsv = self.BGR2HSV(bgr)\n",
    "            h,s,v = cv2.split(hsv)\n",
    "            adjust = random.choice([0.5,1.5])\n",
    "            s = s*adjust\n",
    "            s = np.clip(s, 0, 255).astype(hsv.dtype)\n",
    "            hsv = cv2.merge((h,s,v))\n",
    "            bgr = self.HSV2BGR(hsv)\n",
    "        return bgr\n",
    "\n",
    "    def RandomHue(self,bgr):\n",
    "        if random.random() < 0.5:\n",
    "            hsv = self.BGR2HSV(bgr)\n",
    "            h,s,v = cv2.split(hsv)\n",
    "            adjust = random.choice([0.5,1.5])\n",
    "            h = h*adjust\n",
    "            h = np.clip(h, 0, 255).astype(hsv.dtype)\n",
    "            hsv = cv2.merge((h,s,v))\n",
    "            bgr = self.HSV2BGR(hsv)\n",
    "        return bgr\n",
    "\n",
    "    def randomBlur(self,bgr):\n",
    "        if random.random()<0.5:\n",
    "            bgr = cv2.blur(bgr,(5,5))\n",
    "        return bgr\n",
    "\n",
    "    def randomShift(self,bgr,boxes,labels):\n",
    "        center = (boxes[:,2:]+boxes[:,:2])/2\n",
    "        if random.random() <0.5:\n",
    "            height,width,c = bgr.shape\n",
    "            after_shfit_image = np.zeros((height,width,c),dtype=bgr.dtype)\n",
    "            after_shfit_image[:,:,:] = (104,117,123) #bgr\n",
    "            shift_x = random.uniform(-width*0.2,width*0.2)\n",
    "            shift_y = random.uniform(-height*0.2,height*0.2)\n",
    "            #print(bgr.shape,shift_x,shift_y)\n",
    "            #原图像的平移\n",
    "            if shift_x>=0 and shift_y>=0:\n",
    "                after_shfit_image[int(shift_y):,int(shift_x):,:] = bgr[:height-int(shift_y),:width-int(shift_x),:]\n",
    "            elif shift_x>=0 and shift_y<0:\n",
    "                after_shfit_image[:height+int(shift_y),int(shift_x):,:] = bgr[-int(shift_y):,:width-int(shift_x),:]\n",
    "            elif shift_x <0 and shift_y >=0:\n",
    "                after_shfit_image[int(shift_y):,:width+int(shift_x),:] = bgr[:height-int(shift_y),-int(shift_x):,:]\n",
    "            elif shift_x<0 and shift_y<0:\n",
    "                after_shfit_image[:height+int(shift_y),:width+int(shift_x),:] = bgr[-int(shift_y):,-int(shift_x):,:]\n",
    "\n",
    "            shift_xy = torch.FloatTensor([[int(shift_x),int(shift_y)]]).expand_as(center)\n",
    "            center = center + shift_xy\n",
    "            mask1 = (center[:,0] >0) & (center[:,0] < width)\n",
    "            mask2 = (center[:,1] >0) & (center[:,1] < height)\n",
    "            mask = (mask1 & mask2).view(-1,1)\n",
    "            boxes_in = boxes[mask.expand_as(boxes)].view(-1,4)\n",
    "            if len(boxes_in) == 0:\n",
    "                return bgr,boxes,labels\n",
    "            box_shift = torch.FloatTensor([[int(shift_x),int(shift_y),int(shift_x),int(shift_y)]]).expand_as(boxes_in)\n",
    "            boxes_in = boxes_in+box_shift\n",
    "            labels_in = labels[mask.view(-1)]\n",
    "            return after_shfit_image,boxes_in,labels_in\n",
    "        return bgr,boxes,labels\n",
    "\n",
    "    def randomScale(self,bgr,boxes):\n",
    "        #固定住高度，以0.8-1.2伸缩宽度，做图像形变\n",
    "        if random.random() < 0.5:\n",
    "            scale = random.uniform(0.8,1.2)\n",
    "            height,width,c = bgr.shape\n",
    "            bgr = cv2.resize(bgr,(int(width*scale),height))\n",
    "            scale_tensor = torch.FloatTensor([[scale,1,scale,1]]).expand_as(boxes)\n",
    "            boxes = boxes * scale_tensor\n",
    "            return bgr,boxes\n",
    "        return bgr,boxes\n",
    "\n",
    "    def randomCrop(self,bgr,boxes,labels):\n",
    "        if random.random() < 0.5:\n",
    "            center = (boxes[:,2:]+boxes[:,:2])/2\n",
    "            height,width,c = bgr.shape\n",
    "            h = random.uniform(0.6*height,height)\n",
    "            w = random.uniform(0.6*width,width)\n",
    "            x = random.uniform(0,width-w)\n",
    "            y = random.uniform(0,height-h)\n",
    "            x,y,h,w = int(x),int(y),int(h),int(w)\n",
    "\n",
    "            center = center - torch.FloatTensor([[x,y]]).expand_as(center)\n",
    "            mask1 = (center[:,0]>0) & (center[:,0]<w)\n",
    "            mask2 = (center[:,1]>0) & (center[:,1]<h)\n",
    "            mask = (mask1 & mask2).view(-1,1)\n",
    "\n",
    "            boxes_in = boxes[mask.expand_as(boxes)].view(-1,4)\n",
    "            if(len(boxes_in)==0):\n",
    "                return bgr,boxes,labels\n",
    "            box_shift = torch.FloatTensor([[x,y,x,y]]).expand_as(boxes_in)\n",
    "\n",
    "            boxes_in = boxes_in - box_shift\n",
    "            boxes_in[:,0]=boxes_in[:,0].clamp_(min=0,max=w)\n",
    "            boxes_in[:,2]=boxes_in[:,2].clamp_(min=0,max=w)\n",
    "            boxes_in[:,1]=boxes_in[:,1].clamp_(min=0,max=h)\n",
    "            boxes_in[:,3]=boxes_in[:,3].clamp_(min=0,max=h)\n",
    "\n",
    "            labels_in = labels[mask.view(-1)]\n",
    "            img_croped = bgr[y:y+h,x:x+w,:]\n",
    "            return img_croped,boxes_in,labels_in\n",
    "        return bgr,boxes,labels\n",
    "\n",
    "    def random_flip(self, im, boxes):\n",
    "        if random.random() < 0.5:\n",
    "            im_lr = np.fliplr(im).copy()\n",
    "            h,w,_ = im.shape\n",
    "            xmin = w - boxes[:,2]\n",
    "            xmax = w - boxes[:,0]\n",
    "            boxes[:,0] = xmin\n",
    "            boxes[:,2] = xmax\n",
    "            return im_lr, boxes\n",
    "        return im, boxes\n",
    "\n",
    "    def random_bright(self, im, delta=16): # unused\n",
    "        alpha = random.random()\n",
    "        if alpha > 0.3:\n",
    "            im = im * alpha + random.randrange(-delta,delta)\n",
    "            im = im.clip(min=0,max=255).astype(np.uint8)\n",
    "        return im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KOlUUMjoobF"
   },
   "source": [
    "## Yolov1 Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1664613864126,
     "user": {
      "displayName": "夏宇澄",
      "userId": "12299807091212530744"
     },
     "user_tz": -480
    },
    "id": "kAYSQkK5oyXz"
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class yolov1Loss(nn.Module):\n",
    "    def __init__(self, S, B, C, lambda_coord, lambda_noobj):\n",
    "        # Args:\n",
    "        #    S: size of grid\n",
    "        #    B: number of box\n",
    "        #    C: number of class\n",
    "        super(yolov1Loss, self).__init__()\n",
    "        self.S = S \n",
    "        self.B = B \n",
    "        self.C = C \n",
    "        self.l_coord = lambda_coord\n",
    "        self.l_noobj = lambda_noobj\n",
    "    \n",
    "    def calculateIoU(self, box1, box2):\n",
    "        # calculate the intersection over the union of two sets of boxes, each box contains [xmin,ymin,xmax,ymax]\n",
    "        # Args:\n",
    "        #    size of box1 = [n,4]\n",
    "        #    size of box2 = [m,4]\n",
    "        # Return:\n",
    "        #    size of Iou of two sets of boxes = [n,m]\n",
    "        n = box1.size(0)\n",
    "        m = box2.size(0)\n",
    "        \n",
    "        # take the max of left-bottom point and the min of right-top point \n",
    "        # to calculate the left-top point and the right-bottom point of the intersection\n",
    "        lt = torch.max(\n",
    "            box1[:,:2].unsqueeze(1).expand(n,m,2), # take [xmin,ymin]: [n,2] -> [n,1,2] -> [n,m,2]\n",
    "            box2[:,:2].unsqueeze(0).expand(n,m,2)  # take [xmin,ymin]: [m,2] -> [1,m,2] -> [n,m,2]\n",
    "        )\n",
    "        \n",
    "        rb = torch.min(\n",
    "            box1[:,2:].unsqueeze(1).expand(n,m,2), # take [xmax,ymax]: [n,2] -> [n,1,2] -> [n,m,2]\n",
    "            box2[:,2:].unsqueeze(0).expand(n,m,2)  # take [xmax,ymax]: [m,2] -> [1,m,2] -> [n,m,2]\n",
    "        )\n",
    "        \n",
    "        # calculate weight and height of intersection areas and check if intersection area is 0\n",
    "        wh = rb - lt # [n,m,2]\n",
    "        wh[wh<0] = 0 # if max_left >= min_right or max_bottom >= min_top, then there is no intersection\n",
    "        intersection = wh[:,:,0] * wh[:,:,1] # [n,m]\n",
    "        \n",
    "        area1 = (box1[:,2]-box1[:,0])*(box1[:,3]-box1[:,1])  #[n,]     \n",
    "        area2 = (box2[:,2]-box2[:,0])*(box2[:,3]-box2[:,1])  #[m,]\n",
    "        area1 = area1.unsqueeze(1).expand(n,m) # [n,] -> [n,1] -> [n,m]\n",
    "        area2 = area2.unsqueeze(0).expand(n,m) # [m,] -> [1,m] -> [n,m]\n",
    "        \n",
    "        iou = intersection / (area1 + area2 - intersection)\n",
    "        return iou\n",
    "    def forward(self, preds, targets):\n",
    "        # Args:\n",
    "        #    size of preds = [batchsize, S, S, Bx5+20]: Bx5 means each box has [x,y,w,h,c] 5 values\n",
    "        #    size of targets = [batchsize, S, S, Bx5+20]\n",
    "        S, B, C = self.S, self.B, self.C\n",
    "        N = B * 5 + C \n",
    "        batchsize = preds.size(0)\n",
    "        coord_mask = targets[:,:,:,4] > 0        \n",
    "        noobj_mask = targets[:,:,:,4] == 0\n",
    "        coord_mask = coord_mask.unsqueeze(-1).expand(batchsize, S, S, N)        \n",
    "        noobj_mask = noobj_mask.unsqueeze(-1).expand(batchsize, S, S, N)\n",
    "        \n",
    "        coord_pred = preds[coord_mask].view(-1, N)\n",
    "        box_pred = coord_pred[:,:5*B].contiguous().view(-1, 5)\n",
    "        class_pred = coord_pred[:,5*B:]\n",
    "        \n",
    "        coord_target = targets[coord_mask].view(-1, N)\n",
    "        box_target = coord_target[:,:5*B].contiguous().view(-1, 5)\n",
    "        class_target = coord_target[:,5*B:]\n",
    "        \n",
    "        # compute noobj_loss: only calculate confidence loss\n",
    "        noobj_pred = preds[noobj_mask].view(-1, N)\n",
    "        noobj_target = targets[noobj_mask].view(-1, N)\n",
    "        noobj_pred_mask = torch.cuda.BoolTensor(noobj_pred.size())\n",
    "        noobj_pred_mask.zero_()\n",
    "        for b in range(B):\n",
    "            noobj_pred_mask[:, 4+b*5] = 1\n",
    "        noobj_pred_conf = noobj_pred[noobj_pred_mask]\n",
    "        noobj_target_conf = noobj_target[noobj_pred_mask]  \n",
    "        loss_noobj = F.mse_loss(noobj_pred_conf, noobj_target_conf, reduction = 'sum')\n",
    "        \n",
    "        # compute coord_loss\n",
    "        coord_response_mask = torch.cuda.BoolTensor(box_target.size()).fill_(0) # only compute the loss of the box containing the center of object\n",
    "        box_target_iou = torch.zeros(box_target.size()).cuda()\n",
    "        \n",
    "        # Choose the pred box having the highest IoU for each target boxes\n",
    "        for i in range(0, box_target.size(0), B):\n",
    "            # take all predict boxes at i-th cell\n",
    "            pred_boxes = box_pred[i:i+B]\n",
    "            pred_xyxy = Variable(torch.FloatTensor(pred_boxes.size()))\n",
    "            pred_xyxy[:, :2] = pred_boxes[:, :2]/float(S) - 0.5*pred_boxes[:,2:4]\n",
    "            pred_xyxy[:, 2:4] = pred_boxes[:, :2]/float(S) + 0.5*pred_boxes[:,2:4]   \n",
    "            \n",
    "            # take all target boxes at i-th cell\n",
    "            # Since target boxes contained by each cell are identical in current implement,thus just take the first one\n",
    "            target_boxes = box_target[i].view(-1, 5)\n",
    "            target_xyxy = Variable(torch.FloatTensor(target_boxes.size()))\n",
    "            target_xyxy[:, :2] = target_boxes[:, :2]/float(S) - 0.5*target_boxes[:,2:4]\n",
    "            target_xyxy[:, 2:4] = target_boxes[:, :2]/float(S) + 0.5*target_boxes[:,2:4]\n",
    "                                   \n",
    "            iou = self.calculateIoU(pred_xyxy[:,:4], target_xyxy[:,:4]) # [B,1]\n",
    "            max_iou, max_index = iou.max(0)\n",
    "            max_index = max_index.data.cuda()\n",
    "            \n",
    "            coord_response_mask[i+max_index] = 1\n",
    "            box_target_iou[i+max_index, torch.LongTensor([4]).cuda()] = (max_iou).data.cuda()\n",
    "        \n",
    "        # calculate the loss of the response boxes\n",
    "        box_target_iou = Variable(box_target_iou).cuda()\n",
    "        box_pred_response = box_pred[coord_response_mask].view(-1, 5)\n",
    "        box_target_response = box_target[coord_response_mask].view(-1, 5)\n",
    "        target_iou = box_target_iou[coord_response_mask].view(-1, 5)\n",
    "        loss_xy = F.mse_loss(box_pred_response[:,:2], box_target_response[:,:2], reduction = 'sum')\n",
    "        loss_wh = F.mse_loss(torch.sqrt(box_pred_response[:,2:4]), torch.sqrt(box_target_response[:,2:4]), reduction = 'sum')                \n",
    "        loss_obj = F. mse_loss(box_pred_response[:,4], target_iou[:,4], reduction = 'sum')\n",
    "        \n",
    "        # calculate the class probability loss of cells containing objects\n",
    "        loss_class = F.mse_loss(class_pred, class_target, reduction = 'sum')\n",
    "        \n",
    "        # total loss\n",
    "        loss = self.l_coord * (loss_xy + loss_wh) + loss_obj + self.l_noobj*loss_noobj + loss_class\n",
    "        loss = loss/float(batchsize)\n",
    "                                   \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCt76m6Uot_P"
   },
   "source": [
    "## Training Process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "executionInfo": {
     "elapsed": 38441,
     "status": "error",
     "timestamp": 1664614708565,
     "user": {
      "displayName": "夏宇澄",
      "userId": "12299807091212530744"
     },
     "user_tz": -480
    },
    "id": "ojEJprqbx9QU",
    "outputId": "842f165a-4287-4f3f-eb99-874d5918b349"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dataset has 3508 images\n",
      "the batch_size is 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/anaconda3/envs/Lab_env/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /tmp/pip-req-build-19kunu9c/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Iter [5/439] Loss: 27.4825, average_loss: 25.7259\n",
      "Epoch [1/50], Iter [10/439] Loss: 17.9754, average_loss: 22.3578\n",
      "Epoch [1/50], Iter [15/439] Loss: 19.5806, average_loss: 21.3857\n",
      "Epoch [1/50], Iter [20/439] Loss: 13.4228, average_loss: 20.2970\n",
      "Epoch [1/50], Iter [25/439] Loss: 15.1194, average_loss: 19.3839\n",
      "Epoch [1/50], Iter [30/439] Loss: 13.3412, average_loss: 18.5980\n",
      "Epoch [1/50], Iter [35/439] Loss: 20.5021, average_loss: 18.0731\n",
      "Epoch [1/50], Iter [40/439] Loss: 13.6801, average_loss: 17.4604\n",
      "Epoch [1/50], Iter [45/439] Loss: 13.3594, average_loss: 17.2843\n",
      "Epoch [1/50], Iter [50/439] Loss: 12.3923, average_loss: 16.6869\n",
      "Epoch [1/50], Iter [55/439] Loss: 10.7676, average_loss: 16.0020\n",
      "Epoch [1/50], Iter [60/439] Loss: 8.3666, average_loss: 15.4349\n",
      "Epoch [1/50], Iter [65/439] Loss: 5.0210, average_loss: 14.8832\n",
      "Epoch [1/50], Iter [70/439] Loss: 12.9541, average_loss: 14.6677\n",
      "Epoch [1/50], Iter [75/439] Loss: 5.9694, average_loss: 14.1866\n",
      "Epoch [1/50], Iter [80/439] Loss: 8.4831, average_loss: 13.8725\n",
      "Epoch [1/50], Iter [85/439] Loss: 7.6880, average_loss: 13.5912\n",
      "Epoch [1/50], Iter [90/439] Loss: 8.9678, average_loss: 13.2785\n",
      "Epoch [1/50], Iter [95/439] Loss: 12.9118, average_loss: 13.1072\n",
      "Epoch [1/50], Iter [100/439] Loss: 7.6680, average_loss: 12.7410\n",
      "Epoch [1/50], Iter [105/439] Loss: 7.1373, average_loss: 12.5599\n",
      "Epoch [1/50], Iter [110/439] Loss: 7.9723, average_loss: 12.3021\n",
      "Epoch [1/50], Iter [115/439] Loss: 9.7610, average_loss: 12.1017\n",
      "Epoch [1/50], Iter [120/439] Loss: 8.0200, average_loss: 11.8733\n",
      "Epoch [1/50], Iter [125/439] Loss: 10.5496, average_loss: 11.7739\n",
      "Epoch [1/50], Iter [130/439] Loss: 7.5123, average_loss: 11.5850\n",
      "Epoch [1/50], Iter [135/439] Loss: 9.0184, average_loss: 11.5013\n",
      "Epoch [1/50], Iter [140/439] Loss: 7.2861, average_loss: 11.3387\n",
      "Epoch [1/50], Iter [145/439] Loss: 5.5840, average_loss: 11.1965\n",
      "Epoch [1/50], Iter [150/439] Loss: 6.0926, average_loss: 10.9977\n",
      "Epoch [1/50], Iter [155/439] Loss: 5.3677, average_loss: 10.8432\n",
      "Epoch [1/50], Iter [160/439] Loss: 6.9401, average_loss: 10.7178\n",
      "Epoch [1/50], Iter [165/439] Loss: 4.1510, average_loss: 10.5809\n",
      "Epoch [1/50], Iter [170/439] Loss: 5.4965, average_loss: 10.4386\n",
      "Epoch [1/50], Iter [175/439] Loss: 4.1123, average_loss: 10.2904\n",
      "Epoch [1/50], Iter [180/439] Loss: 5.5564, average_loss: 10.1859\n",
      "Epoch [1/50], Iter [185/439] Loss: 6.9432, average_loss: 10.0792\n",
      "Epoch [1/50], Iter [190/439] Loss: 7.4813, average_loss: 9.9825\n",
      "Epoch [1/50], Iter [195/439] Loss: 5.2683, average_loss: 9.9068\n",
      "Epoch [1/50], Iter [200/439] Loss: 9.0837, average_loss: 9.8351\n",
      "Epoch [1/50], Iter [205/439] Loss: 7.9856, average_loss: 9.7147\n",
      "Epoch [1/50], Iter [210/439] Loss: 4.3039, average_loss: 9.6132\n",
      "Epoch [1/50], Iter [215/439] Loss: 5.3737, average_loss: 9.5307\n",
      "Epoch [1/50], Iter [220/439] Loss: 7.0437, average_loss: 9.4970\n",
      "Epoch [1/50], Iter [225/439] Loss: 4.5570, average_loss: 9.4400\n",
      "Epoch [1/50], Iter [230/439] Loss: 8.3734, average_loss: 9.3848\n",
      "Epoch [1/50], Iter [235/439] Loss: 8.4064, average_loss: 9.3592\n",
      "Epoch [1/50], Iter [240/439] Loss: 6.4209, average_loss: 9.2917\n",
      "Epoch [1/50], Iter [245/439] Loss: 4.8856, average_loss: 9.1941\n",
      "Epoch [1/50], Iter [250/439] Loss: 5.2617, average_loss: 9.1043\n",
      "Epoch [1/50], Iter [255/439] Loss: 4.5077, average_loss: 9.0527\n",
      "Epoch [1/50], Iter [260/439] Loss: 4.5439, average_loss: 9.0007\n",
      "Epoch [1/50], Iter [265/439] Loss: 3.7873, average_loss: 8.9261\n",
      "Epoch [1/50], Iter [270/439] Loss: 4.4975, average_loss: 8.8652\n",
      "Epoch [1/50], Iter [275/439] Loss: 5.8937, average_loss: 8.8346\n",
      "Epoch [1/50], Iter [280/439] Loss: 5.3537, average_loss: 8.7650\n",
      "Epoch [1/50], Iter [285/439] Loss: 6.3589, average_loss: 8.7109\n",
      "Epoch [1/50], Iter [290/439] Loss: 5.4356, average_loss: 8.6481\n",
      "Epoch [1/50], Iter [295/439] Loss: 3.8378, average_loss: 8.5855\n",
      "Epoch [1/50], Iter [300/439] Loss: 5.5642, average_loss: 8.5281\n",
      "Epoch [1/50], Iter [305/439] Loss: 6.0161, average_loss: 8.4768\n",
      "Epoch [1/50], Iter [310/439] Loss: 5.7476, average_loss: 8.4312\n",
      "Epoch [1/50], Iter [315/439] Loss: 3.4044, average_loss: 8.3708\n",
      "Epoch [1/50], Iter [320/439] Loss: 4.1825, average_loss: 8.3098\n",
      "Epoch [1/50], Iter [325/439] Loss: 4.0170, average_loss: 8.2385\n",
      "Epoch [1/50], Iter [330/439] Loss: 5.3619, average_loss: 8.2060\n",
      "Epoch [1/50], Iter [335/439] Loss: 4.6031, average_loss: 8.1649\n",
      "Epoch [1/50], Iter [340/439] Loss: 3.4410, average_loss: 8.1329\n",
      "Epoch [1/50], Iter [345/439] Loss: 9.1409, average_loss: 8.1097\n",
      "Epoch [1/50], Iter [350/439] Loss: 4.8399, average_loss: 8.0654\n",
      "Epoch [1/50], Iter [355/439] Loss: 5.5323, average_loss: 8.0262\n",
      "Epoch [1/50], Iter [360/439] Loss: 3.7327, average_loss: 7.9643\n",
      "Epoch [1/50], Iter [365/439] Loss: 5.5789, average_loss: 7.9147\n",
      "Epoch [1/50], Iter [370/439] Loss: 4.5419, average_loss: 7.8782\n",
      "Epoch [1/50], Iter [375/439] Loss: 4.6700, average_loss: 7.8395\n",
      "Epoch [1/50], Iter [380/439] Loss: 2.7868, average_loss: 7.7913\n",
      "Epoch [1/50], Iter [385/439] Loss: 4.5800, average_loss: 7.7597\n",
      "Epoch [1/50], Iter [390/439] Loss: 5.5402, average_loss: 7.7311\n",
      "Epoch [1/50], Iter [395/439] Loss: 6.2790, average_loss: 7.6996\n",
      "Epoch [1/50], Iter [400/439] Loss: 4.2181, average_loss: 7.6685\n",
      "Epoch [1/50], Iter [405/439] Loss: 6.0373, average_loss: 7.6317\n",
      "Epoch [1/50], Iter [410/439] Loss: 3.1803, average_loss: 7.5900\n",
      "Epoch [1/50], Iter [415/439] Loss: 7.3788, average_loss: 7.5550\n",
      "Epoch [1/50], Iter [420/439] Loss: 5.9250, average_loss: 7.5310\n",
      "Epoch [1/50], Iter [425/439] Loss: 4.4500, average_loss: 7.4901\n",
      "Epoch [1/50], Iter [430/439] Loss: 4.5763, average_loss: 7.4635\n",
      "Epoch [1/50], Iter [435/439] Loss: 3.6902, average_loss: 7.4390\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Variable data has to be a tensor, but got int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 69>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m     validation_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(test_loader)\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest epoch [\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m], average_loss: \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m'\u001b[39m, ep\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, epochs, validation_loss)\n\u001b[0;32m---> 69\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m net\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (images, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_loader):\n\u001b[0;32m---> 59\u001b[0m   images \u001b[38;5;241m=\u001b[39m \u001b[43mVariable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m   target \u001b[38;5;241m=\u001b[39m Variable(target)\n\u001b[1;32m     61\u001b[0m   images, target \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mcuda(), target\u001b[38;5;241m.\u001b[39mcuda()\n",
      "\u001b[0;31mTypeError\u001b[0m: Variable data has to be a tensor, but got int"
     ]
    }
   ],
   "source": [
    "batch_size = 8 #64\n",
    "momentum = 0.9\n",
    "decay = 0.0005\n",
    "epochs = 50\n",
    "\n",
    "file_root = 'VOCdevkit/VOC2007/JPEGImages/'\n",
    "\n",
    "def train():\n",
    "  ## model initiate\n",
    "  learning_rate = 0.001\n",
    "  yolov1 = ResNetYoloV1(50)\n",
    "  yolov1 = load_change_weights(yolov1, 'resnet50')\n",
    "\n",
    "  net = yolov1\n",
    "  net.cuda()\n",
    "  optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum = momentum, weight_decay=decay)\n",
    "  # load data\n",
    "  train_dataset = yoloDataset(root=file_root,list_file=file_root+'voc2007train.txt', train=True,transform = [transforms.ToTensor()] )\n",
    "  train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "  test_dataset = yoloDataset(root=file_root,list_file=file_root+'voc2007valid.txt',train=False,transform = [transforms.ToTensor()] )\n",
    "  test_loader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False,num_workers=4)\n",
    "  print('the dataset has %d images' % (len(train_dataset)))\n",
    "  print('the batch_size is %d' % (batch_size))\n",
    "  # training process\n",
    "  criterion = yolov1Loss(7,2,20,5,0.5)\n",
    "  for ep in range(epochs):\n",
    "    net.train()\n",
    "    if ep >= 2:\n",
    "      learning_rate = 0.01\n",
    "    if ep >= 30:\n",
    "      learning_rate = 0.001\n",
    "    if ep >= 45:\n",
    "      learning_rate = 0.0001\n",
    "    for param_group in optimizer.param_groups:\n",
    "      param_group['lr'] = learning_rate\n",
    "    total_loss = 0.\n",
    "    total_data = 0.\n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "        images = Variable(images)\n",
    "        target = Variable(target)\n",
    "        images,target = images.cuda(),target.cuda()\n",
    "        batch_size_this_iter = images.size(0)\n",
    "\n",
    "        pred = net(images)\n",
    "        loss = criterion(pred,target)\n",
    "        total_loss += loss.item()*batch_size_this_iter\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_data += batch_size_this_iter\n",
    "\n",
    "        if (i+1) % 5 == 0:\n",
    "            print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f, average_loss: %.4f' \n",
    "            %(ep+1, epochs, i+1, len(train_loader), loss.item(), total_loss / total_data))\n",
    "    #validation process\n",
    "    validation_loss = 0.0\n",
    "    net.eval()\n",
    "    for (images, target) in enumerate(test_loader):\n",
    "      images = Variable(images)\n",
    "      target = Variable(target)\n",
    "      images, target = images.cuda(), target.cuda()\n",
    "\n",
    "      pred = net(images)\n",
    "      loss = criterion(pred, target)\n",
    "      validation_loss += loss.item()\n",
    "    validation_loss/=len(test_loader)\n",
    "    print('Test epoch [%d/%d], average_loss: %.4f', ep+1, epochs, validation_loss)\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2ut6a1Ztl24"
   },
   "source": [
    "## Evaluation on VOC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 388,
     "status": "ok",
     "timestamp": 1664613052978,
     "user": {
      "displayName": "夏宇澄",
      "userId": "12299807091212530744"
     },
     "user_tz": -480
    },
    "id": "mZA3yMqntpBV"
   },
   "outputs": [],
   "source": [
    "'''import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"'''\n",
    "import numpy as np\n",
    "VOC_CLASSES = ('aeroplane', 'bicycle', 'bird', 'boat',\n",
    "        'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "        'cow', 'diningtable', 'dog', 'horse',\n",
    "        'motorbike', 'person', 'pottedplant',\n",
    "        'sheep', 'sofa', 'train', 'tvmonitor')\n",
    "\n",
    "def voc_ap(rec,prec,use_07_metric=False):\n",
    "    if use_07_metric:\n",
    "        # 11 point metric\n",
    "        ap = 0.\n",
    "        for t in np.arange(0.,1.1,0.1):\n",
    "            if np.sum(rec >= t) == 0:\n",
    "                p = 0\n",
    "            else:\n",
    "                p = np.max(prec[rec>=t])\n",
    "            ap = ap + p/11.\n",
    "\n",
    "    else:\n",
    "        # correct ap caculation\n",
    "        mrec = np.concatenate(([0.],rec,[1.]))\n",
    "        mpre = np.concatenate(([0.],prec,[0.]))\n",
    "\n",
    "        for i in range(mpre.size -1, 0, -1):\n",
    "            mpre[i-1] = np.maximum(mpre[i-1],mpre[i])\n",
    "\n",
    "        i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "\n",
    "    return ap\n",
    "\n",
    "def voc_eval(preds,target,VOC_CLASSES=VOC_CLASSES,threshold=0.5,use_07_metric=False,):\n",
    "    '''\n",
    "    preds {'cat':[[image_id,confidence,x1,y1,x2,y2],...],'dog':[[],...]}\n",
    "    target {(image_id,class):[[],]}\n",
    "    '''\n",
    "    aps = []\n",
    "    for i,class_ in enumerate(VOC_CLASSES):\n",
    "        pred = preds[class_] #[[image_id,confidence,x1,y1,x2,y2],...]\n",
    "        if len(pred) == 0: #如果这个类别一个都没有检测到的异常情况\n",
    "            ap = -1\n",
    "            print('---class {} ap {}---'.format(class_,ap))\n",
    "            aps += [ap]\n",
    "            break\n",
    "        #print(pred)\n",
    "        image_ids = [x[0] for x in pred]\n",
    "        confidence = np.array([float(x[1]) for x in pred])\n",
    "        BB = np.array([x[2:] for x in pred])\n",
    "        # sort by confidence\n",
    "        sorted_ind = np.argsort(-confidence)\n",
    "        sorted_scores = np.sort(-confidence)\n",
    "        BB = BB[sorted_ind, :]\n",
    "        image_ids = [image_ids[x] for x in sorted_ind]\n",
    "\n",
    "        # go down dets and mark TPs and FPs\n",
    "        npos = 0.\n",
    "        for (key1,key2) in target:\n",
    "            if key2 == class_:\n",
    "                npos += len(target[(key1,key2)]) #统计这个类别的正样本，在这里统计才不会遗漏\n",
    "        nd = len(image_ids)\n",
    "        tp = np.zeros(nd)\n",
    "        fp = np.zeros(nd)\n",
    "        for d,image_id in enumerate(image_ids):\n",
    "            bb = BB[d] #预测框\n",
    "            if (image_id,class_) in target:\n",
    "                BBGT = target[(image_id,class_)] #[[],]\n",
    "                for bbgt in BBGT:\n",
    "                    # compute overlaps\n",
    "                    # intersection\n",
    "                    ixmin = np.maximum(bbgt[0], bb[0])\n",
    "                    iymin = np.maximum(bbgt[1], bb[1])\n",
    "                    ixmax = np.minimum(bbgt[2], bb[2])\n",
    "                    iymax = np.minimum(bbgt[3], bb[3])\n",
    "                    iw = np.maximum(ixmax - ixmin + 1., 0.)\n",
    "                    ih = np.maximum(iymax - iymin + 1., 0.)\n",
    "                    inters = iw * ih\n",
    "\n",
    "                    union = (bb[2]-bb[0]+1.)*(bb[3]-bb[1]+1.) + (bbgt[2]-bbgt[0]+1.)*(bbgt[3]-bbgt[1]+1.) - inters\n",
    "                    if union == 0:\n",
    "                        print(bb,bbgt)\n",
    "                    \n",
    "                    overlaps = inters/union\n",
    "                    if overlaps > threshold:\n",
    "                        tp[d] = 1\n",
    "                        BBGT.remove(bbgt) #这个框已经匹配到了，不能再匹配\n",
    "                        if len(BBGT) == 0:\n",
    "                            del target[(image_id,class_)] #删除没有box的键值\n",
    "                        break\n",
    "                fp[d] = 1-tp[d]\n",
    "            else:\n",
    "                fp[d] = 1\n",
    "        fp = np.cumsum(fp)\n",
    "        tp = np.cumsum(tp)\n",
    "        rec = tp/float(npos)\n",
    "        prec = tp/np.maximum(tp + fp, np.finfo(np.float64).eps)\n",
    "        #print(rec,prec)\n",
    "        ap = voc_ap(rec, prec, use_07_metric)\n",
    "        print('---class {} ap {}---'.format(class_,ap))\n",
    "        aps += [ap]\n",
    "    print('---map {}---'.format(np.mean(aps)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "1KOlUUMjoobF"
   ],
   "provenance": [
    {
     "file_id": "1TP-dps8rtpZ2G9mVvXEF7WXBuUV1oJFZ",
     "timestamp": 1662903786776
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
