{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKvr9DC6x9QM"
   },
   "source": [
    "# Object Detection\n",
    "* 2 people in a group\n",
    "* Deadline: 10/13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zPl8L7qx9QP"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "- PASCAL VOC 2007\n",
    "  - Number of class: 20\n",
    "  - The data list is provided in the google drive. However, you have to download the training/testing data from http://host.robots.ox.ac.uk/pascal/VOC/voc2007/. \n",
    "    - Train/Val data: 5011\n",
    "        - Each row contains one image and its bounding boxes.\n",
    "        - filename ($x_{min}$, $y_{min}$, $x_{max}$, $y_{max}$, $label$) $\\times$ object_num\n",
    "        - class idx starts from 1\n",
    "    - Test data: 4951\n",
    "        - filename ($x_{min}$, $y_{min}$, $x_{max}$, $y_{max}$, $label$) $\\times$ object_num\n",
    "        - class idx starts from 0\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXto_Mqqx9QQ"
   },
   "source": [
    "### Loading your data into Google Colab with Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20080,
     "status": "ok",
     "timestamp": 1664613762229,
     "user": {
      "displayName": "夏宇澄",
      "userId": "12299807091212530744"
     },
     "user_tz": -480
    },
    "id": "uZ4-AYFzx9QQ",
    "outputId": "303d704d-b754-426b-8e91-5a7e34f1b342"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knUoH4P4x9QR"
   },
   "source": [
    "## Resnet50 backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 285,
     "status": "ok",
     "timestamp": 1664613766309,
     "user": {
      "displayName": "夏宇澄",
      "userId": "12299807091212530744"
     },
     "user_tz": -480
    },
    "id": "ykAGukXsx9QS"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.resnet import BasicBlock, Bottleneck\n",
    "from torchvision.models.resnet import model_urls\n",
    "from torchsummary import summary\n",
    "\n",
    "class classify_bottleneck(nn.Module):\n",
    "  expansion = 1\n",
    "\n",
    "  def __init__(self, inplanes, planes, stride=1, block_type='A'):\n",
    "    super(classify_bottleneck, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "    self.bn1 = nn.BatchNorm2d(planes)\n",
    "    self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=2, bias=False,dilation=2)\n",
    "    self.bn2 = nn.BatchNorm2d(planes)\n",
    "    self.conv3 = nn.Conv2d(planes, planes, kernel_size=1, bias=False)\n",
    "    self.bn3 = nn.BatchNorm2d(planes)\n",
    "\n",
    "    self.downsample = nn.Sequential()\n",
    "    if stride != 1 or block_type=='B':\n",
    "        self.downsample = nn.Sequential(\n",
    "            nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "            nn.BatchNorm2d(self.expansion*planes)\n",
    "        )\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = F.relu(self.bn1(self.conv1(x)))\n",
    "    out = F.relu(self.bn2(self.conv2(out)))\n",
    "    out = self.bn3(self.conv3(out))\n",
    "    out += self.downsample(x)\n",
    "    out = F.relu(out)\n",
    "    return out\n",
    "\n",
    "class ResNetYoloV1(nn.Module):\n",
    "\n",
    "    def __init__(self, resnet_type):\n",
    "\t\n",
    "        resnet_spec = {18: (BasicBlock, [2, 2, 2, 2], [64, 64, 128, 256, 512], 'resnet18'),\n",
    "\t\t       34: (BasicBlock, [3, 4, 6, 3], [64, 64, 128, 256, 512], 'resnet34'),\n",
    "\t\t       50: (Bottleneck, [3, 4, 6, 3], [64, 256, 512, 1024, 2048], 'resnet50'),\n",
    "\t\t       101: (Bottleneck, [3, 4, 23, 3], [64, 256, 512, 1024, 2048], 'resnet101'),\n",
    "\t\t       152: (Bottleneck, [3, 8, 36, 3], [64, 256, 512, 1024, 2048], 'resnet152')}\n",
    "        block, layers, channels, name = resnet_spec[resnet_type]\n",
    "        \n",
    "        self.name = name\n",
    "        self.inplanes = 64\n",
    "        super(ResNetYoloV1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        self.layer5 = self._make_classify_layer(in_channels=2048) #2048*14*14\n",
    "\n",
    "        self.conv_end = nn.Conv2d(256, 30, kernel_size=3, stride=2, padding=1, bias=False)#30*7*7\n",
    "        self.bn_end = nn.BatchNorm2d(30)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.001)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _make_classify_layer(self,in_channels):\n",
    "        layers = []\n",
    "        layers.append(classify_bottleneck(inplanes=in_channels, planes=256, block_type='B'))\n",
    "        layers.append(classify_bottleneck(inplanes=256, planes=256))\n",
    "        layers.append(classify_bottleneck(inplanes=256, planes=256))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.layer2(x1)\n",
    "        x3 = self.layer3(x2)\n",
    "        x4 = self.layer4(x3)\n",
    "        # x4 layer output size: (B, 2048, 7, 7)\n",
    "        x5 = self.layer5(x4)\n",
    "        x = self.conv_end(x5)\n",
    "        x = self.bn_end(x)\n",
    "        x = torch.sigmoid(x) #归一化到0-1\n",
    "        # x = x.view(-1,7,7,30)\n",
    "        x = x.permute(0,2,3,1) #(-1,7,7,30)\n",
    "        return x\n",
    "\n",
    "    def init_weights(self):\n",
    "        org_resnet = torch.utils.model_zoo.load_url(model_urls[self.name])\n",
    "        # drop orginal resnet fc layer, add 'None' in case of no fc layer, that will raise error\n",
    "        org_resnet.pop('fc.weight', None)\n",
    "        org_resnet.pop('fc.bias', None)\n",
    "\n",
    "        self.load_state_dict(org_resnet)\n",
    "        print(\"Initialize resnet from model zoo\")\n",
    "\n",
    "def load_change_weights(model, model_name):\n",
    "  \n",
    "  org_resnet = torch.utils.model_zoo.load_url(model_urls[model_name])\n",
    "  org_resnet.pop('fc.weight', None)\n",
    "  org_resnet.pop('fc.bias', None)\n",
    "\n",
    "  dd = model.state_dict()\n",
    "  for k in org_resnet.keys():\n",
    "      # print(k)\n",
    "      if k in dd.keys() and not k.startswith('fc'):\n",
    "          # print('yes')\n",
    "          dd[k] = org_resnet[k]\n",
    "  model.load_state_dict(dd)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kj7b4Idx9QT"
   },
   "source": [
    "### Assignment\n",
    "You are required to build a model to perform object detection on the provided Pascal VOC dataset in this project.\n",
    "Here are some hints that help you to accomplish the project successfully.\n",
    "\n",
    "### Hints\n",
    "- YOLOv1 is the simplest and suggested model to be implemented.\n",
    "- Be careful of the normalization techniques on bounding boxes.\n",
    "    1. normalize the height and width with image size to fall into 0 and 1\n",
    "    2. x and y coordinates are parameterized to be the offsets of a particular grid cell and also bounded by 0 and 1\n",
    "- Loss function has a great impact on training stability.\n",
    "    1. loss function is the most important in this project, especially in calculating IOU\n",
    "    2. only one bounding box predictor is responsible for each object\n",
    "    3. weights for different types of losses\n",
    "    4. predict the square root of height and width instead of predicting them directly\n",
    "- Data augmentation.\n",
    "    1. It contains only 5011 images in total. Furthermore, the labels are highly imbalanced.\n",
    "    2. Random scaling and translations are applied when training YOLO.\n",
    "    3. Note that the bounding box coordinates have to be changed accordingly if the image was transformed.\n",
    "\n",
    "### Evaluation Metric\n",
    "- Please evaluate your model on Pascal VOC testing set using Mean Average Precision (mAP).\n",
    "- Write a brief report including your implementation, performance and  qualitative results(visualize bounding box on some images). \n",
    "- For more detailed explanation of mAP, please follow https://github.com/rafaelpadilla/Object-Detection-Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1664613312162,
     "user": {
      "displayName": "夏宇澄",
      "userId": "12299807091212530744"
     },
     "user_tz": -480
    },
    "id": "57fbGJtQdhTA"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MRA4OTgoeYy"
   },
   "source": [
    "## Dataset & data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 351,
     "status": "ok",
     "timestamp": 1664613768703,
     "user": {
      "displayName": "夏宇澄",
      "userId": "12299807091212530744"
     },
     "user_tz": -480
    },
    "id": "-unQlvZTocGb"
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class yoloDataset(data.Dataset):\n",
    "    image_size = 448 # Size to be aligned\n",
    "\n",
    "    # Parsing data list\n",
    "    def __init__(self,root,list_file,train,transform):\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.fnames = []\n",
    "        self.boxes = []\n",
    "        self.labels = []\n",
    "        self.mean = (123,117,104) # RGB\n",
    "\n",
    "        # Cat multiple list files together.\n",
    "        '''if isinstance(list_file, list):\n",
    "            # This is especially useful for voc07/voc12 combination.\n",
    "            tmp_file = '/tmp/listfile.txt'\n",
    "            os.system('cat %s > %s' % (' '.join(list_file), tmp_file))\n",
    "            list_file = tmp_file'''\n",
    "\n",
    "        with open(list_file) as f:\n",
    "            lines  = f.readlines()\n",
    "\n",
    "        # format of each line: filename (x_min, y_min, x_max, y_max, label) * object_num\n",
    "        for line in lines:\n",
    "            splited = line.strip().split() # .strip(): reomove space, tab from the end of each line\n",
    "            self.fnames.append(splited[0])\n",
    "            num_boxes = (len(splited) - 1) // 5\n",
    "            box=[]\n",
    "            label=[]\n",
    "            for i in range(num_boxes):\n",
    "                x = float(splited[1+5*i])\n",
    "                y = float(splited[2+5*i])\n",
    "                x2 = float(splited[3+5*i])\n",
    "                y2 = float(splited[4+5*i])\n",
    "                c = splited[5+5*i]\n",
    "                box.append([x,y,x2,y2])\n",
    "                label.append(int(c)+1) # +1: since the idx start from 0\n",
    "            self.boxes.append(torch.Tensor(box))\n",
    "            self.labels.append(torch.LongTensor(label))\n",
    "        self.num_samples = len(self.boxes)\n",
    "\n",
    "    # Getting single transformed, preprocessed image and its target\n",
    "    def __getitem__(self,idx):\n",
    "        fname = self.fnames[idx]\n",
    "        img = cv2.imread(os.path.join(self.root+fname))\n",
    "        boxes = self.boxes[idx].clone()\n",
    "        labels = self.labels[idx].clone()\n",
    "\n",
    "        # Randomly transforming image\n",
    "        if self.train:\n",
    "            #img = self.random_bright(img)\n",
    "            img, boxes = self.random_flip(img, boxes)\n",
    "            img,boxes = self.randomScale(img,boxes)\n",
    "            img = self.randomBlur(img)\n",
    "            img = self.RandomBrightness(img)\n",
    "            img = self.RandomHue(img)\n",
    "            img = self.RandomSaturation(img)\n",
    "            img,boxes,labels = self.randomShift(img,boxes,labels)\n",
    "            img,boxes,labels = self.randomCrop(img,boxes,labels)\n",
    "\n",
    "        # #debug: showing the transformed image\n",
    "        # box_show = boxes.numpy().reshape(-1)\n",
    "        # # print(box_show)\n",
    "        # img_show = self.BGR2RGB(img)\n",
    "        # pt1=(int(box_show[0]),int(box_show[1])); pt2=(int(box_show[2]),int(box_show[3]))\n",
    "        # cv2.rectangle(img_show,pt1=pt1,pt2=pt2,color=(0,255,0),thickness=1)\n",
    "        # plt.figure()\n",
    "        \n",
    "        # plt.imshow(img_show)\n",
    "        # plt.show()\n",
    "        # #debug\n",
    "\n",
    "        h,w,_ = img.shape\n",
    "        boxes /= torch.Tensor([w,h,w,h]).expand_as(boxes) \n",
    "        # .expand_as(other): expand this tensor as other\n",
    "        # [w, h, w, h] (1, 4) will be expanded to (#box, 4)\n",
    "\n",
    "        img = self.BGR2RGB(img) # because pytorch pretrained model use RGB\n",
    "        img = self.subMean(img,self.mean)\n",
    "        img = cv2.resize(img,(self.image_size,self.image_size))\n",
    "        target = self.encoder(boxes,labels) # 7x7x30, where 30 = 5*2(xywh+confidence for 2 boxes) + 20(classes)\n",
    "        for t in self.transform:\n",
    "            img = t(img)\n",
    "\n",
    "        return img,target\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    # Utils\n",
    "    # Encoding the boxes, labels for single image\n",
    "    def encoder(self,boxes,labels):\n",
    "        grid_num = 7\n",
    "        target = torch.zeros((grid_num,grid_num,30))\n",
    "        cell_size = 1./grid_num\n",
    "        wh = boxes[:,2:]-boxes[:,:2]\n",
    "        cxcy = (boxes[:,2:]+boxes[:,:2])/2\n",
    "        for i in range(cxcy.size()[0]):\n",
    "            cxcy_sample = cxcy[i]\n",
    "            ij = (cxcy_sample/cell_size).ceil()-1 #\n",
    "            target[int(ij[1]),int(ij[0]),4] = 1\n",
    "            target[int(ij[1]),int(ij[0]),9] = 1\n",
    "            target[int(ij[1]),int(ij[0]),int(labels[i])+9] = 1\n",
    "            xy = ij*cell_size # upper left coordinates of corresponding grid\n",
    "            delta_xy = (cxcy_sample -xy)/cell_size\n",
    "            target[int(ij[1]),int(ij[0]),2:4] = wh[i]\n",
    "            target[int(ij[1]),int(ij[0]),:2] = delta_xy\n",
    "            target[int(ij[1]),int(ij[0]),7:9] = wh[i]\n",
    "            target[int(ij[1]),int(ij[0]),5:7] = delta_xy\n",
    "        return target\n",
    "\n",
    "    def BGR2RGB(self,img):\n",
    "        return cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    def BGR2HSV(self,img):\n",
    "        return cv2.cvtColor(img,cv2.COLOR_BGR2HSV)\n",
    "    def HSV2BGR(self,img):\n",
    "        return cv2.cvtColor(img,cv2.COLOR_HSV2BGR)\n",
    "    \n",
    "    def subMean(self,bgr,mean):\n",
    "        mean = np.array(mean, dtype=np.float32)\n",
    "        bgr = bgr - mean\n",
    "        return bgr\n",
    "    \n",
    "    def RandomBrightness(self,bgr):\n",
    "        if random.random() < 0.5:\n",
    "            hsv = self.BGR2HSV(bgr)\n",
    "            h,s,v = cv2.split(hsv)\n",
    "            adjust = random.choice([0.5,1.5])\n",
    "            v = v*adjust\n",
    "            v = np.clip(v, 0, 255).astype(hsv.dtype)\n",
    "            hsv = cv2.merge((h,s,v))\n",
    "            bgr = self.HSV2BGR(hsv)\n",
    "        return bgr\n",
    "\n",
    "    def RandomSaturation(self,bgr):\n",
    "        if random.random() < 0.5:\n",
    "            hsv = self.BGR2HSV(bgr)\n",
    "            h,s,v = cv2.split(hsv)\n",
    "            adjust = random.choice([0.5,1.5])\n",
    "            s = s*adjust\n",
    "            s = np.clip(s, 0, 255).astype(hsv.dtype)\n",
    "            hsv = cv2.merge((h,s,v))\n",
    "            bgr = self.HSV2BGR(hsv)\n",
    "        return bgr\n",
    "\n",
    "    def RandomHue(self,bgr):\n",
    "        if random.random() < 0.5:\n",
    "            hsv = self.BGR2HSV(bgr)\n",
    "            h,s,v = cv2.split(hsv)\n",
    "            adjust = random.choice([0.5,1.5])\n",
    "            h = h*adjust\n",
    "            h = np.clip(h, 0, 255).astype(hsv.dtype)\n",
    "            hsv = cv2.merge((h,s,v))\n",
    "            bgr = self.HSV2BGR(hsv)\n",
    "        return bgr\n",
    "\n",
    "    def randomBlur(self,bgr):\n",
    "        if random.random()<0.5:\n",
    "            bgr = cv2.blur(bgr,(5,5))\n",
    "        return bgr\n",
    "\n",
    "    def randomShift(self,bgr,boxes,labels):\n",
    "        center = (boxes[:,2:]+boxes[:,:2])/2\n",
    "        if random.random() <0.5:\n",
    "            height,width,c = bgr.shape\n",
    "            after_shfit_image = np.zeros((height,width,c),dtype=bgr.dtype)\n",
    "            after_shfit_image[:,:,:] = (104,117,123) #bgr\n",
    "            shift_x = random.uniform(-width*0.2,width*0.2)\n",
    "            shift_y = random.uniform(-height*0.2,height*0.2)\n",
    "            #print(bgr.shape,shift_x,shift_y)\n",
    "            #原图像的平移\n",
    "            if shift_x>=0 and shift_y>=0:\n",
    "                after_shfit_image[int(shift_y):,int(shift_x):,:] = bgr[:height-int(shift_y),:width-int(shift_x),:]\n",
    "            elif shift_x>=0 and shift_y<0:\n",
    "                after_shfit_image[:height+int(shift_y),int(shift_x):,:] = bgr[-int(shift_y):,:width-int(shift_x),:]\n",
    "            elif shift_x <0 and shift_y >=0:\n",
    "                after_shfit_image[int(shift_y):,:width+int(shift_x),:] = bgr[:height-int(shift_y),-int(shift_x):,:]\n",
    "            elif shift_x<0 and shift_y<0:\n",
    "                after_shfit_image[:height+int(shift_y),:width+int(shift_x),:] = bgr[-int(shift_y):,-int(shift_x):,:]\n",
    "\n",
    "            shift_xy = torch.FloatTensor([[int(shift_x),int(shift_y)]]).expand_as(center)\n",
    "            center = center + shift_xy\n",
    "            mask1 = (center[:,0] >0) & (center[:,0] < width)\n",
    "            mask2 = (center[:,1] >0) & (center[:,1] < height)\n",
    "            mask = (mask1 & mask2).view(-1,1)\n",
    "            boxes_in = boxes[mask.expand_as(boxes)].view(-1,4)\n",
    "            if len(boxes_in) == 0:\n",
    "                return bgr,boxes,labels\n",
    "            box_shift = torch.FloatTensor([[int(shift_x),int(shift_y),int(shift_x),int(shift_y)]]).expand_as(boxes_in)\n",
    "            boxes_in = boxes_in+box_shift\n",
    "            labels_in = labels[mask.view(-1)]\n",
    "            return after_shfit_image,boxes_in,labels_in\n",
    "        return bgr,boxes,labels\n",
    "\n",
    "    def randomScale(self,bgr,boxes):\n",
    "        #固定住高度，以0.8-1.2伸缩宽度，做图像形变\n",
    "        if random.random() < 0.5:\n",
    "            scale = random.uniform(0.8,1.2)\n",
    "            height,width,c = bgr.shape\n",
    "            bgr = cv2.resize(bgr,(int(width*scale),height))\n",
    "            scale_tensor = torch.FloatTensor([[scale,1,scale,1]]).expand_as(boxes)\n",
    "            boxes = boxes * scale_tensor\n",
    "            return bgr,boxes\n",
    "        return bgr,boxes\n",
    "\n",
    "    def randomCrop(self,bgr,boxes,labels):\n",
    "        if random.random() < 0.5:\n",
    "            center = (boxes[:,2:]+boxes[:,:2])/2\n",
    "            height,width,c = bgr.shape\n",
    "            h = random.uniform(0.6*height,height)\n",
    "            w = random.uniform(0.6*width,width)\n",
    "            x = random.uniform(0,width-w)\n",
    "            y = random.uniform(0,height-h)\n",
    "            x,y,h,w = int(x),int(y),int(h),int(w)\n",
    "\n",
    "            center = center - torch.FloatTensor([[x,y]]).expand_as(center)\n",
    "            mask1 = (center[:,0]>0) & (center[:,0]<w)\n",
    "            mask2 = (center[:,1]>0) & (center[:,1]<h)\n",
    "            mask = (mask1 & mask2).view(-1,1)\n",
    "\n",
    "            boxes_in = boxes[mask.expand_as(boxes)].view(-1,4)\n",
    "            if(len(boxes_in)==0):\n",
    "                return bgr,boxes,labels\n",
    "            box_shift = torch.FloatTensor([[x,y,x,y]]).expand_as(boxes_in)\n",
    "\n",
    "            boxes_in = boxes_in - box_shift\n",
    "            boxes_in[:,0]=boxes_in[:,0].clamp_(min=0,max=w)\n",
    "            boxes_in[:,2]=boxes_in[:,2].clamp_(min=0,max=w)\n",
    "            boxes_in[:,1]=boxes_in[:,1].clamp_(min=0,max=h)\n",
    "            boxes_in[:,3]=boxes_in[:,3].clamp_(min=0,max=h)\n",
    "\n",
    "            labels_in = labels[mask.view(-1)]\n",
    "            img_croped = bgr[y:y+h,x:x+w,:]\n",
    "            return img_croped,boxes_in,labels_in\n",
    "        return bgr,boxes,labels\n",
    "\n",
    "    def random_flip(self, im, boxes):\n",
    "        if random.random() < 0.5:\n",
    "            im_lr = np.fliplr(im).copy()\n",
    "            h,w,_ = im.shape\n",
    "            xmin = w - boxes[:,2]\n",
    "            xmax = w - boxes[:,0]\n",
    "            boxes[:,0] = xmin\n",
    "            boxes[:,2] = xmax\n",
    "            return im_lr, boxes\n",
    "        return im, boxes\n",
    "\n",
    "    def random_bright(self, im, delta=16): # unused\n",
    "        alpha = random.random()\n",
    "        if alpha > 0.3:\n",
    "            im = im * alpha + random.randrange(-delta,delta)\n",
    "            im = im.clip(min=0,max=255).astype(np.uint8)\n",
    "        return im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KOlUUMjoobF"
   },
   "source": [
    "## Yolov1 Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1664613864126,
     "user": {
      "displayName": "夏宇澄",
      "userId": "12299807091212530744"
     },
     "user_tz": -480
    },
    "id": "kAYSQkK5oyXz"
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class yolov1Loss(nn.Module):\n",
    "    def __init__(self, S, B, C, lambda_coord, lambda_noobj):\n",
    "        # Args:\n",
    "        #    S: size of grid\n",
    "        #    B: number of box\n",
    "        #    C: number of class\n",
    "        super(yolov1Loss, self).__init__()\n",
    "        self.S = S \n",
    "        self.B = B \n",
    "        self.C = C \n",
    "        self.l_coord = lambda_coord\n",
    "        self.l_noobj = lambda_noobj\n",
    "    \n",
    "    def calculateIoU(self, box1, box2):\n",
    "        # calculate the intersection over the union of two sets of boxes, each box contains [xmin,ymin,xmax,ymax]\n",
    "        # Args:\n",
    "        #    size of box1 = [n,4]\n",
    "        #    size of box2 = [m,4]\n",
    "        # Return:\n",
    "        #    size of Iou of two sets of boxes = [n,m]\n",
    "        n = box1.size(0)\n",
    "        m = box2.size(0)\n",
    "        \n",
    "        # take the max of left-bottom point and the min of right-top point \n",
    "        # to calculate the left-top point and the right-bottom point of the intersection\n",
    "        lt = torch.max(\n",
    "            box1[:,:2].unsqueeze(1).expand(n,m,2), # take [xmin,ymin]: [n,2] -> [n,1,2] -> [n,m,2]\n",
    "            box2[:,:2].unsqueeze(0).expand(n,m,2)  # take [xmin,ymin]: [m,2] -> [1,m,2] -> [n,m,2]\n",
    "        )\n",
    "        \n",
    "        rb = torch.min(\n",
    "            box1[:,2:].unsqueeze(1).expand(n,m,2), # take [xmax,ymax]: [n,2] -> [n,1,2] -> [n,m,2]\n",
    "            box2[:,2:].unsqueeze(0).expand(n,m,2)  # take [xmax,ymax]: [m,2] -> [1,m,2] -> [n,m,2]\n",
    "        )\n",
    "        \n",
    "        # calculate weight and height of intersection areas and check if intersection area is 0\n",
    "        wh = rb - lt # [n,m,2]\n",
    "        wh[wh<0] = 0 # if max_left >= min_right or max_bottom >= min_top, then there is no intersection\n",
    "        intersection = wh[:,:,0] * wh[:,:,1] # [n,m]\n",
    "        \n",
    "        area1 = (box1[:,2]-box1[:,0])*(box1[:,3]-box1[:,1])  #[n,]     \n",
    "        area2 = (box2[:,2]-box2[:,0])*(box2[:,3]-box2[:,1])  #[m,]\n",
    "        area1 = area1.unsqueeze(1).expand(n,m) # [n,] -> [n,1] -> [n,m]\n",
    "        area2 = area2.unsqueeze(0).expand(n,m) # [m,] -> [1,m] -> [n,m]\n",
    "        \n",
    "        iou = intersection / (area1 + area2 - intersection)\n",
    "        return iou\n",
    "    def forward(self, preds, targets):\n",
    "        # Args:\n",
    "        #    size of preds = [batchsize, S, S, Bx5+20]: Bx5 means each box has [x,y,w,h,c] 5 values\n",
    "        #    size of targets = [batchsize, S, S, Bx5+20]\n",
    "        S, B, C = self.S, self.B, self.C\n",
    "        N = B * 5 + C \n",
    "        batchsize = preds.size(0)\n",
    "        coord_mask = targets[:,:,:,4] > 0        \n",
    "        noobj_mask = targets[:,:,:,4] == 0\n",
    "        coord_mask = coord_mask.unsqueeze(-1).expand(batchsize, S, S, N)        \n",
    "        noobj_mask = noobj_mask.unsqueeze(-1).expand(batchsize, S, S, N)\n",
    "        \n",
    "        coord_pred = preds[coord_mask].view(-1, N)\n",
    "        box_pred = coord_pred[:,:5*B].contiguous().view(-1, 5)\n",
    "        class_pred = coord_pred[:,5*B:]\n",
    "        \n",
    "        coord_target = targets[coord_mask].view(-1, N)\n",
    "        box_target = coord_target[:,:5*B].contiguous().view(-1, 5)\n",
    "        class_target = coord_target[:,5*B:]\n",
    "        \n",
    "        # compute noobj_loss: only calculate confidence loss\n",
    "        noobj_pred = preds[noobj_mask].view(-1, N)\n",
    "        noobj_target = targets[noobj_mask].view(-1, N)\n",
    "        noobj_pred_mask = torch.cuda.BoolTensor(noobj_pred.size())\n",
    "        noobj_pred_mask.zero_()\n",
    "        for b in range(B):\n",
    "            noobj_pred_mask[:, 4+b*5] = 1\n",
    "        noobj_pred_conf = noobj_pred[noobj_pred_mask]\n",
    "        noobj_target_conf = noobj_target[noobj_pred_mask]  \n",
    "        loss_noobj = F.mse_loss(noobj_pred_conf, noobj_target_conf, reduction = 'sum')\n",
    "        \n",
    "        # compute coord_loss\n",
    "        coord_response_mask = torch.cuda.BoolTensor(box_target.size()).fill_(0) # only compute the loss of the box containing the center of object\n",
    "        box_target_iou = torch.zeros(box_target.size()).cuda()\n",
    "        \n",
    "        # Choose the pred box having the highest IoU for each target boxes\n",
    "        for i in range(0, box_target.size(0), B):\n",
    "            # take all predict boxes at i-th cell\n",
    "            pred_boxes = box_pred[i:i+B]\n",
    "            pred_xyxy = Variable(torch.FloatTensor(pred_boxes.size()))\n",
    "            pred_xyxy[:, :2] = pred_boxes[:, :2]/float(S) - 0.5*pred_boxes[:,2:4]\n",
    "            pred_xyxy[:, 2:4] = pred_boxes[:, :2]/float(S) + 0.5*pred_boxes[:,2:4]   \n",
    "            \n",
    "            # take all target boxes at i-th cell\n",
    "            # Since target boxes contained by each cell are identical in current implement,thus just take the first one\n",
    "            target_boxes = box_target[i].view(-1, 5)\n",
    "            target_xyxy = Variable(torch.FloatTensor(target_boxes.size()))\n",
    "            target_xyxy[:, :2] = target_boxes[:, :2]/float(S) - 0.5*target_boxes[:,2:4]\n",
    "            target_xyxy[:, 2:4] = target_boxes[:, :2]/float(S) + 0.5*target_boxes[:,2:4]\n",
    "                                   \n",
    "            iou = self.calculateIoU(pred_xyxy[:,:4], target_xyxy[:,:4]) # [B,1]\n",
    "            max_iou, max_index = iou.max(0)\n",
    "            max_index = max_index.data.cuda()\n",
    "            \n",
    "            coord_response_mask[i+max_index] = 1\n",
    "            box_target_iou[i+max_index, torch.LongTensor([4]).cuda()] = (max_iou).data.cuda()\n",
    "        \n",
    "        # calculate the loss of the response boxes\n",
    "        box_target_iou = Variable(box_target_iou).cuda()\n",
    "        box_pred_response = box_pred[coord_response_mask].view(-1, 5)\n",
    "        box_target_response = box_target[coord_response_mask].view(-1, 5)\n",
    "        target_iou = box_target_iou[coord_response_mask].view(-1, 5)\n",
    "        loss_xy = F.mse_loss(box_pred_response[:,:2], box_target_response[:,:2], reduction = 'sum')\n",
    "        loss_wh = F.mse_loss(torch.sqrt(box_pred_response[:,2:4]), torch.sqrt(box_target_response[:,2:4]), reduction = 'sum')                \n",
    "        loss_obj = F. mse_loss(box_pred_response[:,4], target_iou[:,4], reduction = 'sum')\n",
    "        \n",
    "        # calculate the class probability loss of cells containing objects\n",
    "        loss_class = F.mse_loss(class_pred, class_target, reduction = 'sum')\n",
    "        \n",
    "        # total loss\n",
    "        loss = self.l_coord * (loss_xy + loss_wh) + loss_obj + self.l_noobj*loss_noobj + loss_class\n",
    "        loss = loss/float(batchsize)\n",
    "                                   \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCt76m6Uot_P"
   },
   "source": [
    "## Training Process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "executionInfo": {
     "elapsed": 38441,
     "status": "error",
     "timestamp": 1664614708565,
     "user": {
      "displayName": "夏宇澄",
      "userId": "12299807091212530744"
     },
     "user_tz": -480
    },
    "id": "ojEJprqbx9QU",
    "outputId": "842f165a-4287-4f3f-eb99-874d5918b349"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dataset has 5010 images\n",
      "the batch_size is 8\n",
      "Epoch [1/50], Iter [5/627] Loss: 20.0711, average_loss: 23.9822\n",
      "Epoch [1/50], Iter [10/627] Loss: 21.8621, average_loss: 22.4023\n",
      "Epoch [1/50], Iter [15/627] Loss: 20.8807, average_loss: 21.1255\n",
      "Epoch [1/50], Iter [20/627] Loss: 14.5793, average_loss: 19.8461\n",
      "Epoch [1/50], Iter [25/627] Loss: 15.6770, average_loss: 19.0664\n",
      "Epoch [1/50], Iter [30/627] Loss: 16.4223, average_loss: 18.2948\n",
      "Epoch [1/50], Iter [35/627] Loss: 16.9417, average_loss: 17.6521\n",
      "Epoch [1/50], Iter [40/627] Loss: 14.5529, average_loss: 16.8268\n",
      "Epoch [1/50], Iter [45/627] Loss: 25.2479, average_loss: 16.4145\n",
      "Epoch [1/50], Iter [50/627] Loss: 9.7385, average_loss: 15.7722\n",
      "Epoch [1/50], Iter [55/627] Loss: 13.8839, average_loss: 15.2486\n",
      "Epoch [1/50], Iter [60/627] Loss: 8.6060, average_loss: 14.6320\n",
      "Epoch [1/50], Iter [65/627] Loss: 9.0687, average_loss: 14.2661\n",
      "Epoch [1/50], Iter [70/627] Loss: 12.9680, average_loss: 14.0402\n",
      "Epoch [1/50], Iter [75/627] Loss: 12.2042, average_loss: 13.7973\n",
      "Epoch [1/50], Iter [80/627] Loss: 15.2748, average_loss: 13.6213\n",
      "Epoch [1/50], Iter [85/627] Loss: 5.2095, average_loss: 13.3429\n",
      "Epoch [1/50], Iter [90/627] Loss: 12.7050, average_loss: 13.0445\n",
      "Epoch [1/50], Iter [95/627] Loss: 6.5835, average_loss: 12.7917\n",
      "Epoch [1/50], Iter [100/627] Loss: 11.8615, average_loss: 12.5836\n",
      "Epoch [1/50], Iter [105/627] Loss: 6.9269, average_loss: 12.3819\n",
      "Epoch [1/50], Iter [110/627] Loss: 6.7719, average_loss: 12.1163\n",
      "Epoch [1/50], Iter [115/627] Loss: 11.6028, average_loss: 11.9771\n",
      "Epoch [1/50], Iter [120/627] Loss: 6.5349, average_loss: 11.8081\n",
      "Epoch [1/50], Iter [125/627] Loss: 4.5958, average_loss: 11.6309\n",
      "Epoch [1/50], Iter [130/627] Loss: 11.4721, average_loss: 11.5125\n",
      "Epoch [1/50], Iter [135/627] Loss: 9.3913, average_loss: 11.3440\n",
      "Epoch [1/50], Iter [140/627] Loss: 4.8099, average_loss: 11.1891\n",
      "Epoch [1/50], Iter [145/627] Loss: 10.9475, average_loss: 11.1464\n",
      "Epoch [1/50], Iter [150/627] Loss: 5.5959, average_loss: 11.0109\n",
      "Epoch [1/50], Iter [155/627] Loss: 17.3757, average_loss: 10.9395\n",
      "Epoch [1/50], Iter [160/627] Loss: 6.8464, average_loss: 10.7858\n",
      "Epoch [1/50], Iter [165/627] Loss: 7.4554, average_loss: 10.7063\n",
      "Epoch [1/50], Iter [170/627] Loss: 8.4493, average_loss: 10.5917\n",
      "Epoch [1/50], Iter [175/627] Loss: 5.1426, average_loss: 10.4473\n",
      "Epoch [1/50], Iter [180/627] Loss: 4.5133, average_loss: 10.3457\n",
      "Epoch [1/50], Iter [185/627] Loss: 6.8284, average_loss: 10.2678\n",
      "Epoch [1/50], Iter [190/627] Loss: 5.8150, average_loss: 10.1428\n",
      "Epoch [1/50], Iter [195/627] Loss: 5.0547, average_loss: 10.0049\n",
      "Epoch [1/50], Iter [200/627] Loss: 3.9283, average_loss: 9.9188\n",
      "Epoch [1/50], Iter [205/627] Loss: 6.0846, average_loss: 9.8352\n",
      "Epoch [1/50], Iter [210/627] Loss: 4.0541, average_loss: 9.7201\n",
      "Epoch [1/50], Iter [215/627] Loss: 9.5035, average_loss: 9.6395\n",
      "Epoch [1/50], Iter [220/627] Loss: 3.7549, average_loss: 9.5621\n",
      "Epoch [1/50], Iter [225/627] Loss: 6.8847, average_loss: 9.4743\n",
      "Epoch [1/50], Iter [230/627] Loss: 5.9481, average_loss: 9.3891\n",
      "Epoch [1/50], Iter [235/627] Loss: 5.2844, average_loss: 9.2914\n",
      "Epoch [1/50], Iter [240/627] Loss: 4.3339, average_loss: 9.2014\n",
      "Epoch [1/50], Iter [245/627] Loss: 6.1388, average_loss: 9.1281\n",
      "Epoch [1/50], Iter [250/627] Loss: 6.8815, average_loss: 9.0934\n",
      "Epoch [1/50], Iter [255/627] Loss: 6.1848, average_loss: 9.0553\n",
      "Epoch [1/50], Iter [260/627] Loss: 4.0999, average_loss: 8.9770\n",
      "Epoch [1/50], Iter [265/627] Loss: 4.8359, average_loss: 8.9002\n",
      "Epoch [1/50], Iter [270/627] Loss: 4.7247, average_loss: 8.8328\n",
      "Epoch [1/50], Iter [275/627] Loss: 3.1850, average_loss: 8.7733\n",
      "Epoch [1/50], Iter [280/627] Loss: 6.3480, average_loss: 8.7381\n",
      "Epoch [1/50], Iter [285/627] Loss: 3.5473, average_loss: 8.6971\n",
      "Epoch [1/50], Iter [290/627] Loss: 4.6975, average_loss: 8.6476\n",
      "Epoch [1/50], Iter [295/627] Loss: 6.1337, average_loss: 8.5989\n",
      "Epoch [1/50], Iter [300/627] Loss: 4.2371, average_loss: 8.5384\n",
      "Epoch [1/50], Iter [305/627] Loss: 5.0917, average_loss: 8.5105\n",
      "Epoch [1/50], Iter [310/627] Loss: 4.1383, average_loss: 8.4367\n",
      "Epoch [1/50], Iter [315/627] Loss: 5.9645, average_loss: 8.3867\n",
      "Epoch [1/50], Iter [320/627] Loss: 9.0176, average_loss: 8.3528\n",
      "Epoch [1/50], Iter [325/627] Loss: 4.9402, average_loss: 8.2933\n",
      "Epoch [1/50], Iter [330/627] Loss: 5.3752, average_loss: 8.2337\n",
      "Epoch [1/50], Iter [335/627] Loss: 5.7607, average_loss: 8.1763\n",
      "Epoch [1/50], Iter [340/627] Loss: 4.1656, average_loss: 8.1249\n",
      "Epoch [1/50], Iter [345/627] Loss: 3.2960, average_loss: 8.0812\n",
      "Epoch [1/50], Iter [350/627] Loss: 7.5192, average_loss: 8.0469\n",
      "Epoch [1/50], Iter [355/627] Loss: 6.2925, average_loss: 8.0054\n",
      "Epoch [1/50], Iter [360/627] Loss: 4.9178, average_loss: 7.9587\n",
      "Epoch [1/50], Iter [365/627] Loss: 3.3289, average_loss: 7.9260\n",
      "Epoch [1/50], Iter [370/627] Loss: 5.2467, average_loss: 7.8886\n",
      "Epoch [1/50], Iter [375/627] Loss: 5.7168, average_loss: 7.8606\n",
      "Epoch [1/50], Iter [380/627] Loss: 3.9853, average_loss: 7.8161\n",
      "Epoch [1/50], Iter [385/627] Loss: 11.8554, average_loss: 7.8054\n",
      "Epoch [1/50], Iter [390/627] Loss: 6.7119, average_loss: 7.7773\n",
      "Epoch [1/50], Iter [395/627] Loss: 3.1410, average_loss: 7.7358\n",
      "Epoch [1/50], Iter [400/627] Loss: 4.3437, average_loss: 7.7015\n",
      "Epoch [1/50], Iter [405/627] Loss: 8.3958, average_loss: 7.6701\n",
      "Epoch [1/50], Iter [410/627] Loss: 6.5235, average_loss: 7.6383\n",
      "Epoch [1/50], Iter [415/627] Loss: 4.1690, average_loss: 7.5964\n",
      "Epoch [1/50], Iter [420/627] Loss: 5.6096, average_loss: 7.5592\n",
      "Epoch [1/50], Iter [425/627] Loss: 3.5728, average_loss: 7.5282\n",
      "Epoch [1/50], Iter [430/627] Loss: 4.8769, average_loss: 7.5093\n",
      "Epoch [1/50], Iter [435/627] Loss: 4.4944, average_loss: 7.4757\n",
      "Epoch [1/50], Iter [440/627] Loss: 4.1342, average_loss: 7.4425\n",
      "Epoch [1/50], Iter [445/627] Loss: 4.2728, average_loss: 7.4101\n",
      "Epoch [1/50], Iter [450/627] Loss: 2.9762, average_loss: 7.3841\n",
      "Epoch [1/50], Iter [455/627] Loss: 4.9983, average_loss: 7.3541\n",
      "Epoch [1/50], Iter [460/627] Loss: 3.8209, average_loss: 7.3345\n",
      "Epoch [1/50], Iter [465/627] Loss: 2.5415, average_loss: 7.2939\n",
      "Epoch [1/50], Iter [470/627] Loss: 6.3144, average_loss: 7.2672\n",
      "Epoch [1/50], Iter [475/627] Loss: 4.3580, average_loss: 7.2458\n",
      "Epoch [1/50], Iter [480/627] Loss: 6.0063, average_loss: 7.2118\n",
      "Epoch [1/50], Iter [485/627] Loss: 3.4912, average_loss: 7.1803\n",
      "Epoch [1/50], Iter [490/627] Loss: 3.5525, average_loss: 7.1650\n",
      "Epoch [1/50], Iter [495/627] Loss: 4.9737, average_loss: 7.1396\n",
      "Epoch [1/50], Iter [500/627] Loss: 5.2155, average_loss: 7.1074\n",
      "Epoch [1/50], Iter [505/627] Loss: 4.5180, average_loss: 7.0870\n",
      "Epoch [1/50], Iter [510/627] Loss: 5.9111, average_loss: 7.0685\n",
      "Epoch [1/50], Iter [515/627] Loss: 3.7863, average_loss: 7.0548\n",
      "Epoch [1/50], Iter [520/627] Loss: 6.6863, average_loss: 7.0318\n",
      "Epoch [1/50], Iter [525/627] Loss: 6.3251, average_loss: 7.0154\n",
      "Epoch [1/50], Iter [530/627] Loss: 3.8193, average_loss: 6.9882\n",
      "Epoch [1/50], Iter [535/627] Loss: 6.3237, average_loss: 6.9640\n",
      "Epoch [1/50], Iter [540/627] Loss: 4.2507, average_loss: 6.9305\n",
      "Epoch [1/50], Iter [545/627] Loss: 5.3238, average_loss: 6.9064\n",
      "Epoch [1/50], Iter [550/627] Loss: 2.8677, average_loss: 6.8807\n",
      "Epoch [1/50], Iter [555/627] Loss: 4.0246, average_loss: 6.8557\n",
      "Epoch [1/50], Iter [560/627] Loss: 4.6965, average_loss: 6.8349\n",
      "Epoch [1/50], Iter [565/627] Loss: 6.9524, average_loss: 6.8190\n",
      "Epoch [1/50], Iter [570/627] Loss: 3.7023, average_loss: 6.7965\n",
      "Epoch [1/50], Iter [575/627] Loss: 5.2146, average_loss: 6.7834\n",
      "Epoch [1/50], Iter [580/627] Loss: 3.2723, average_loss: 6.7682\n",
      "Epoch [1/50], Iter [585/627] Loss: 8.2414, average_loss: 6.7519\n",
      "Epoch [1/50], Iter [590/627] Loss: 3.7094, average_loss: 6.7314\n",
      "Epoch [1/50], Iter [595/627] Loss: 4.8735, average_loss: 6.7132\n",
      "Epoch [1/50], Iter [600/627] Loss: 4.0693, average_loss: 6.6911\n",
      "Epoch [1/50], Iter [605/627] Loss: 3.5199, average_loss: 6.6690\n",
      "Epoch [1/50], Iter [610/627] Loss: 3.6234, average_loss: 6.6505\n",
      "Epoch [1/50], Iter [615/627] Loss: 3.0623, average_loss: 6.6290\n",
      "Epoch [1/50], Iter [620/627] Loss: 3.4936, average_loss: 6.6054\n",
      "Epoch [1/50], Iter [625/627] Loss: 3.0595, average_loss: 6.5830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@1643.273] global /io/opencv/modules/imgcodecs/src/loadsave.cpp (239) findDecoder imread_('VOCdevkit/VOC2007/JPEGImages/002824.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@1643.274] global /io/opencv/modules/imgcodecs/src/loadsave.cpp (239) findDecoder imread_('VOCdevkit/VOC2007/JPEGImages/005505.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@1643.274] global /io/opencv/modules/imgcodecs/src/loadsave.cpp (239) findDecoder imread_('VOCdevkit/VOC2007/JPEGImages/007962.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@1643.275] global /io/opencv/modules/imgcodecs/src/loadsave.cpp (239) findDecoder imread_('VOCdevkit/VOC2007/JPEGImages/001912.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@1643.275] global /io/opencv/modules/imgcodecs/src/loadsave.cpp (239) findDecoder imread_('VOCdevkit/VOC2007/JPEGImages/003490.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@1643.275] global /io/opencv/modules/imgcodecs/src/loadsave.cpp (239) findDecoder imread_('VOCdevkit/VOC2007/JPEGImages/006970.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@1643.278] global /io/opencv/modules/imgcodecs/src/loadsave.cpp (239) findDecoder imread_('VOCdevkit/VOC2007/JPEGImages/005200.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@1643.279] global /io/opencv/modules/imgcodecs/src/loadsave.cpp (239) findDecoder imread_('VOCdevkit/VOC2007/JPEGImages/001183.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@1643.279] global /io/opencv/modules/imgcodecs/src/loadsave.cpp (239) findDecoder imread_('VOCdevkit/VOC2007/JPEGImages/009003.jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/andrea/anaconda3/envs/Lab_env/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/andrea/anaconda3/envs/Lab_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/andrea/anaconda3/envs/Lab_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_298275/510945355.py\", line 83, in __getitem__\n    h,w,_ = img.shape\nAttributeError: 'NoneType' object has no attribute 'shape'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 68>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m     validation_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(test_loader)\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest epoch [\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m], average_loss: \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m'\u001b[39m, ep\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, epochs, validation_loss)\n\u001b[0;32m---> 68\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m validation_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     56\u001b[0m net\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (images, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_loader):\n\u001b[1;32m     58\u001b[0m   images \u001b[38;5;241m=\u001b[39m Variable(images)\n\u001b[1;32m     59\u001b[0m   target \u001b[38;5;241m=\u001b[39m Variable(target)\n",
      "File \u001b[0;32m~/anaconda3/envs/Lab_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 521\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    524\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/Lab_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1203\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Lab_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1229\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1229\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/Lab_env/lib/python3.9/site-packages/torch/_utils.py:425\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexc_type, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;66;03m# Some exceptions have first argument as non-str but explicitly\u001b[39;00m\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;66;03m# have message field\u001b[39;00m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexc_type(message\u001b[38;5;241m=\u001b[39mmsg)\n\u001b[0;32m--> 425\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexc_type(msg)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/andrea/anaconda3/envs/Lab_env/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/andrea/anaconda3/envs/Lab_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/andrea/anaconda3/envs/Lab_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_298275/510945355.py\", line 83, in __getitem__\n    h,w,_ = img.shape\nAttributeError: 'NoneType' object has no attribute 'shape'\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8 #64\n",
    "momentum = 0.9\n",
    "decay = 0.0005\n",
    "epochs = 50\n",
    "\n",
    "file_root = 'VOCdevkit/VOC2007/JPEGImages/'\n",
    "\n",
    "def train():\n",
    "  ## model initiate\n",
    "  learning_rate = 0.001\n",
    "  yolov1 = ResNetYoloV1(50)\n",
    "  yolov1 = load_change_weights(yolov1, 'resnet50')\n",
    "\n",
    "  net = yolov1\n",
    "  net.cuda()\n",
    "  optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum = momentum, weight_decay=decay)\n",
    "  # load data\n",
    "  train_dataset = yoloDataset(root=file_root,list_file=file_root+'voc2007.txt', train=True,transform = [transforms.ToTensor()] )\n",
    "  train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "  test_dataset = yoloDataset(root=file_root,list_file=file_root+'voc2007test.txt',train=False,transform = [transforms.ToTensor()] )\n",
    "  test_loader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False,num_workers=4)\n",
    "  print('the dataset has %d images' % (len(train_dataset)))\n",
    "  print('the batch_size is %d' % (batch_size))\n",
    "  # training process\n",
    "  net.train()\n",
    "  criterion = yolov1Loss(7,2,20,5,0.5)\n",
    "  for ep in range(epochs):\n",
    "    if ep >= 2:\n",
    "      learning_rate = 0.01\n",
    "    if ep >= 30:\n",
    "      learning_rate = 0.001\n",
    "    if ep >= 45:\n",
    "      learning_rate = 0.0001\n",
    "    for param_group in optimizer.param_groups:\n",
    "      param_group['lr'] = learning_rate\n",
    "    total_loss = 0.\n",
    "\n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "        images = Variable(images)\n",
    "        target = Variable(target)\n",
    "        images,target = images.cuda(),target.cuda()\n",
    "        \n",
    "        pred = net(images)\n",
    "        loss = criterion(pred,target)\n",
    "#         total_loss += loss.data[0]\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 5 == 0:\n",
    "            print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f, average_loss: %.4f' \n",
    "            %(ep+1, epochs, i+1, len(train_loader), loss.item(), total_loss / (i+1)))\n",
    "    #validation process\n",
    "    validation_loss = 0.0\n",
    "    net.eval()\n",
    "    for (images, target) in enumerate(test_loader):\n",
    "      images = Variable(images)\n",
    "      target = Variable(target)\n",
    "      images, target = images.cuda(), target.cuda()\n",
    "\n",
    "      pred = net(images)\n",
    "      loss = criterion(pred, target)\n",
    "      validation_loss += loss.item()\n",
    "    validation_loss/=len(test_loader)\n",
    "    print('Test epoch [%d/%d], average_loss: %.4f', ep+1, epochs, validation_loss)\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2ut6a1Ztl24"
   },
   "source": [
    "## Evaluation on VOC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 388,
     "status": "ok",
     "timestamp": 1664613052978,
     "user": {
      "displayName": "夏宇澄",
      "userId": "12299807091212530744"
     },
     "user_tz": -480
    },
    "id": "mZA3yMqntpBV"
   },
   "outputs": [],
   "source": [
    "'''import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"'''\n",
    "import numpy as np\n",
    "VOC_CLASSES = ('aeroplane', 'bicycle', 'bird', 'boat',\n",
    "        'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "        'cow', 'diningtable', 'dog', 'horse',\n",
    "        'motorbike', 'person', 'pottedplant',\n",
    "        'sheep', 'sofa', 'train', 'tvmonitor')\n",
    "\n",
    "def voc_ap(rec,prec,use_07_metric=False):\n",
    "    if use_07_metric:\n",
    "        # 11 point metric\n",
    "        ap = 0.\n",
    "        for t in np.arange(0.,1.1,0.1):\n",
    "            if np.sum(rec >= t) == 0:\n",
    "                p = 0\n",
    "            else:\n",
    "                p = np.max(prec[rec>=t])\n",
    "            ap = ap + p/11.\n",
    "\n",
    "    else:\n",
    "        # correct ap caculation\n",
    "        mrec = np.concatenate(([0.],rec,[1.]))\n",
    "        mpre = np.concatenate(([0.],prec,[0.]))\n",
    "\n",
    "        for i in range(mpre.size -1, 0, -1):\n",
    "            mpre[i-1] = np.maximum(mpre[i-1],mpre[i])\n",
    "\n",
    "        i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "\n",
    "    return ap\n",
    "\n",
    "def voc_eval(preds,target,VOC_CLASSES=VOC_CLASSES,threshold=0.5,use_07_metric=False,):\n",
    "    '''\n",
    "    preds {'cat':[[image_id,confidence,x1,y1,x2,y2],...],'dog':[[],...]}\n",
    "    target {(image_id,class):[[],]}\n",
    "    '''\n",
    "    aps = []\n",
    "    for i,class_ in enumerate(VOC_CLASSES):\n",
    "        pred = preds[class_] #[[image_id,confidence,x1,y1,x2,y2],...]\n",
    "        if len(pred) == 0: #如果这个类别一个都没有检测到的异常情况\n",
    "            ap = -1\n",
    "            print('---class {} ap {}---'.format(class_,ap))\n",
    "            aps += [ap]\n",
    "            break\n",
    "        #print(pred)\n",
    "        image_ids = [x[0] for x in pred]\n",
    "        confidence = np.array([float(x[1]) for x in pred])\n",
    "        BB = np.array([x[2:] for x in pred])\n",
    "        # sort by confidence\n",
    "        sorted_ind = np.argsort(-confidence)\n",
    "        sorted_scores = np.sort(-confidence)\n",
    "        BB = BB[sorted_ind, :]\n",
    "        image_ids = [image_ids[x] for x in sorted_ind]\n",
    "\n",
    "        # go down dets and mark TPs and FPs\n",
    "        npos = 0.\n",
    "        for (key1,key2) in target:\n",
    "            if key2 == class_:\n",
    "                npos += len(target[(key1,key2)]) #统计这个类别的正样本，在这里统计才不会遗漏\n",
    "        nd = len(image_ids)\n",
    "        tp = np.zeros(nd)\n",
    "        fp = np.zeros(nd)\n",
    "        for d,image_id in enumerate(image_ids):\n",
    "            bb = BB[d] #预测框\n",
    "            if (image_id,class_) in target:\n",
    "                BBGT = target[(image_id,class_)] #[[],]\n",
    "                for bbgt in BBGT:\n",
    "                    # compute overlaps\n",
    "                    # intersection\n",
    "                    ixmin = np.maximum(bbgt[0], bb[0])\n",
    "                    iymin = np.maximum(bbgt[1], bb[1])\n",
    "                    ixmax = np.minimum(bbgt[2], bb[2])\n",
    "                    iymax = np.minimum(bbgt[3], bb[3])\n",
    "                    iw = np.maximum(ixmax - ixmin + 1., 0.)\n",
    "                    ih = np.maximum(iymax - iymin + 1., 0.)\n",
    "                    inters = iw * ih\n",
    "\n",
    "                    union = (bb[2]-bb[0]+1.)*(bb[3]-bb[1]+1.) + (bbgt[2]-bbgt[0]+1.)*(bbgt[3]-bbgt[1]+1.) - inters\n",
    "                    if union == 0:\n",
    "                        print(bb,bbgt)\n",
    "                    \n",
    "                    overlaps = inters/union\n",
    "                    if overlaps > threshold:\n",
    "                        tp[d] = 1\n",
    "                        BBGT.remove(bbgt) #这个框已经匹配到了，不能再匹配\n",
    "                        if len(BBGT) == 0:\n",
    "                            del target[(image_id,class_)] #删除没有box的键值\n",
    "                        break\n",
    "                fp[d] = 1-tp[d]\n",
    "            else:\n",
    "                fp[d] = 1\n",
    "        fp = np.cumsum(fp)\n",
    "        tp = np.cumsum(tp)\n",
    "        rec = tp/float(npos)\n",
    "        prec = tp/np.maximum(tp + fp, np.finfo(np.float64).eps)\n",
    "        #print(rec,prec)\n",
    "        ap = voc_ap(rec, prec, use_07_metric)\n",
    "        print('---class {} ap {}---'.format(class_,ap))\n",
    "        aps += [ap]\n",
    "    print('---map {}---'.format(np.mean(aps)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "1KOlUUMjoobF"
   ],
   "provenance": [
    {
     "file_id": "1TP-dps8rtpZ2G9mVvXEF7WXBuUV1oJFZ",
     "timestamp": 1662903786776
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
