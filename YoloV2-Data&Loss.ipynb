{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19357,
     "status": "ok",
     "timestamp": 1665204208865,
     "user": {
      "displayName": "夏宇澄",
      "userId": "12299807091212530744"
     },
     "user_tz": -480
    },
    "id": "G9LrlYojsDK0",
    "outputId": "fd18c7f6-08ae-49c3-8a34-63d5c53f6d11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqmLN702siby"
   },
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1054,
     "status": "ok",
     "timestamp": 1665204211531,
     "user": {
      "displayName": "夏宇澄",
      "userId": "12299807091212530744"
     },
     "user_tz": -480
    },
    "id": "mYeUxuSYetGP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import uniform\n",
    "import cv2\n",
    "\n",
    "class Compose(object):\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, data):\n",
    "        for function_ in self.transforms:\n",
    "            data = function_(data)\n",
    "        return data\n",
    "\n",
    "\n",
    "class Crop(object):\n",
    "\n",
    "    def __init__(self, max_crop=0.1):\n",
    "        super().__init__()\n",
    "        self.max_crop = max_crop\n",
    "\n",
    "    def __call__(self, data):\n",
    "        image, label = data\n",
    "        height, width = image.shape[:2]\n",
    "        xmin = width\n",
    "        ymin = height\n",
    "        xmax = 0\n",
    "        ymax = 0\n",
    "        for lb in label:\n",
    "            xmin = min(xmin, lb[0])\n",
    "            ymin = min(ymin, lb[1])\n",
    "            xmax = max(xmax, lb[2])\n",
    "            ymax = max(ymax, lb[2])\n",
    "        cropped_left = uniform(0, self.max_crop)\n",
    "        cropped_right = uniform(0, self.max_crop)\n",
    "        cropped_top = uniform(0, self.max_crop)\n",
    "        cropped_bottom = uniform(0, self.max_crop)\n",
    "        new_xmin = int(min(cropped_left * width, xmin))\n",
    "        new_ymin = int(min(cropped_top * height, ymin))\n",
    "        new_xmax = int(max(width - 1 - cropped_right * width, xmax))\n",
    "        new_ymax = int(max(height - 1 - cropped_bottom * height, ymax))\n",
    "\n",
    "        image = image[new_ymin:new_ymax, new_xmin:new_xmax, :]\n",
    "        label = [[lb[0] - new_xmin, lb[1] - new_ymin, lb[2] - new_xmin, lb[3] - new_ymin, lb[4]] for lb in label]\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class VerticalFlip(object):\n",
    "\n",
    "    def __init__(self, prob=0.5):\n",
    "        super().__init__()\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, data):\n",
    "        image, label = data\n",
    "        if uniform(0, 1) >= self.prob:\n",
    "            image = cv2.flip(image, 1)\n",
    "            width = image.shape[1]\n",
    "            label = [[width - lb[2], lb[1], width - lb[0], lb[3], lb[4]] for lb in label]\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class HSVAdjust(object):\n",
    "\n",
    "    def __init__(self, hue=30, saturation=1.5, value=1.5, prob=0.5):\n",
    "        super().__init__()\n",
    "        self.hue = hue\n",
    "        self.saturation = saturation\n",
    "        self.value = value\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, data):\n",
    "\n",
    "        def clip_hue(hue_channel):\n",
    "            hue_channel[hue_channel >= 360] -= 360\n",
    "            hue_channel[hue_channel < 0] += 360\n",
    "            return hue_channel\n",
    "\n",
    "        image, label = data\n",
    "        adjust_hue = uniform(-self.hue, self.hue)\n",
    "        adjust_saturation = uniform(1, self.saturation)\n",
    "        if uniform(0, 1) >= self.prob:\n",
    "            adjust_saturation = 1 / adjust_saturation\n",
    "        adjust_value = uniform(1, self.value)\n",
    "        if uniform(0, 1) >= self.prob:\n",
    "            adjust_value = 1 / adjust_value\n",
    "        image = image.astype(np.float32) / 255\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "        image[:, :, 0] += adjust_hue\n",
    "        image[:, :, 0] = clip_hue(image[:, :, 0])\n",
    "        image[:, :, 1] = np.clip(adjust_saturation * image[:, :, 1], 0.0, 1.0)\n",
    "        image[:, :, 2] = np.clip(adjust_value * image[:, :, 2], 0.0, 1.0)\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_HSV2RGB)\n",
    "        image = (image * 255).astype(np.float32)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class Resize(object):\n",
    "\n",
    "    def __init__(self, image_size):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def __call__(self, data):\n",
    "        image, label = data\n",
    "        height, width = image.shape[:2]\n",
    "        image = cv2.resize(image, (self.image_size, self.image_size))\n",
    "        width_ratio = float(self.image_size) / width\n",
    "        height_ratio = float(self.image_size) / height\n",
    "        new_label = []\n",
    "        for lb in label:\n",
    "            resized_xmin = lb[0] * width_ratio\n",
    "            resized_ymin = lb[1] * height_ratio\n",
    "            resized_xmax = lb[2] * width_ratio\n",
    "            resized_ymax = lb[3] * height_ratio\n",
    "            resize_width = resized_xmax - resized_xmin\n",
    "            resize_height = resized_ymax - resized_ymin\n",
    "            new_label.append([resized_xmin, resized_ymin, resize_width, resize_height, lb[4]])\n",
    "\n",
    "        return image, new_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axBHzXSCvvJo"
   },
   "source": [
    "## Dataset of VOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 451,
     "status": "ok",
     "timestamp": 1665204214075,
     "user": {
      "displayName": "夏宇澄",
      "userId": "12299807091212530744"
     },
     "user_tz": -480
    },
    "id": "Ie6nUUVOvxKt"
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class yoloDataset(data.Dataset):\n",
    "    image_size = 416 # Size to be aligned\n",
    "\n",
    "    # Parsing data list\n",
    "    def __init__(self,root,list_file,train):\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.fnames = []\n",
    "        self.objects = [] # [x_min, y_min, x_max, y_max, class]\n",
    "\n",
    "        with open(list_file) as f:\n",
    "            lines  = f.readlines()\n",
    "\n",
    "        # format of each line: filename (x_min, y_min, x_max, y_max, label) * object_num\n",
    "        for line in lines:\n",
    "            splited = line.strip().split() # .strip(): reomove space, tab from the end of each line\n",
    "            self.fnames.append(splited[0])\n",
    "            num_boxes = (len(splited) - 1) // 5\n",
    "            one_object = []\n",
    "            for i in range(num_boxes):\n",
    "                x_min = float(splited[1+5*i])\n",
    "                y_min = float(splited[2+5*i])\n",
    "                x_max = float(splited[3+5*i])\n",
    "                y_max = float(splited[4+5*i])\n",
    "                c = splited[5+5*i]\n",
    "                one_object.append([x_min,y_min,x_max,y_max, int(c)+1])\n",
    "            self.objects.append(one_object)\n",
    "        self.num_samples = len(self.objects)\n",
    "\n",
    "    # Getting single transformed, preprocessed image and its bounding boxes\n",
    "    def __getitem__(self,idx):\n",
    "        fname = self.fnames[idx]\n",
    "        img = cv2.imread(os.path.join(self.root+fname))\n",
    "        image_objects = self.objects[idx]\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.train:\n",
    "            transformations = Compose([HSVAdjust(), VerticalFlip(), Crop(), Resize(self.image_size)])\n",
    "        else:\n",
    "            transformations = Compose([Resize(self.image_size)])\n",
    "\n",
    "        # h,w,_ = img.shape\n",
    "        # boxes /= torch.Tensor([w,h,w,h]).expand_as(boxes)    -> replaced by batch-norm(?)\n",
    "        # img = self.subMean(img,self.mean)\n",
    "        # to see the whih grid is the box is located at\n",
    "        # .expand_as(other): expand this tensor as other\n",
    "        # [w, h, w, h] (1, 4) will be expanded to (#box, 4)\n",
    "        img, image_objects = transformations((img, image_objects))\n",
    "\n",
    "        # img.shape (416,416,3) -> (3,416,416)\n",
    "        return np.transpose(np.array(img, dtype=np.float32), (2, 0, 1)), np.array(image_objects, dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 272,
     "status": "ok",
     "timestamp": 1665204285720,
     "user": {
      "displayName": "夏宇澄",
      "userId": "12299807091212530744"
     },
     "user_tz": -480
    },
    "id": "G4P9NfvjOeN6",
    "outputId": "9a18de72-e32c-4065-dae7-98e32ee4183a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 40.40618 ,  22.078213, 375.5938  , 393.92178 ,   3.      ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test dataset\n",
    "root = '/content/drive/MyDrive/Colab Notebooks/JPEGImages/'\n",
    "dataset = yoloDataset(root=root, list_file=root+'voc2007.txt', train=True)\n",
    "dataset.__getitem__(0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKetdfDlpilF"
   },
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3AItS8RvpoKV"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class YoloLoss(nn.modules.loss._Loss):\n",
    "    # The loss I borrow from LightNet repo.\n",
    "    def __init__(self, num_classes, anchors, reduction=32, coord_scale=1.0, noobject_scale=1.0,\n",
    "                 object_scale=5.0, class_scale=1.0, thresh=0.6):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_anchors = len(anchors)\n",
    "        self.anchor_step = len(anchors[0])\n",
    "        self.anchors = torch.Tensor(anchors)\n",
    "        self.reduction = reduction\n",
    "\n",
    "        self.coord_scale = coord_scale\n",
    "        self.noobject_scale = noobject_scale\n",
    "        self.object_scale = object_scale\n",
    "        self.class_scale = class_scale\n",
    "        self.thresh = thresh\n",
    "\n",
    "    def forward(self, output, target):\n",
    "\n",
    "        batch_size = output.data.size(0)\n",
    "        height = output.data.size(2)\n",
    "        width = output.data.size(3)\n",
    "\n",
    "        # Get x,y,w,h,conf,cls\n",
    "        output = output.view(batch_size, self.num_anchors, -1, height * width)\n",
    "        coord = torch.zeros_like(output[:, :, :4, :])\n",
    "        coord[:, :, :2, :] = output[:, :, :2, :].sigmoid()  \n",
    "        coord[:, :, 2:4, :] = output[:, :, 2:4, :]\n",
    "        conf = output[:, :, 4, :].sigmoid()\n",
    "        cls = output[:, :, 5:, :].contiguous().view(batch_size * self.num_anchors, self.num_classes,\n",
    "                                                    height * width).transpose(1, 2).contiguous().view(-1,\n",
    "                                                                                                      self.num_classes)\n",
    "\n",
    "        # Create prediction boxes\n",
    "        pred_boxes = torch.FloatTensor(batch_size * self.num_anchors * height * width, 4)\n",
    "        lin_x = torch.range(0, width - 1).repeat(height, 1).view(height * width)\n",
    "        lin_y = torch.range(0, height - 1).repeat(width, 1).t().contiguous().view(height * width)\n",
    "        anchor_w = self.anchors[:, 0].contiguous().view(self.num_anchors, 1)\n",
    "        anchor_h = self.anchors[:, 1].contiguous().view(self.num_anchors, 1)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            pred_boxes = pred_boxes.cuda()\n",
    "            lin_x = lin_x.cuda()\n",
    "            lin_y = lin_y.cuda()\n",
    "            anchor_w = anchor_w.cuda()\n",
    "            anchor_h = anchor_h.cuda()\n",
    "\n",
    "        pred_boxes[:, 0] = (coord[:, :, 0].detach() + lin_x).view(-1)\n",
    "        pred_boxes[:, 1] = (coord[:, :, 1].detach() + lin_y).view(-1)\n",
    "        pred_boxes[:, 2] = (coord[:, :, 2].detach().exp() * anchor_w).view(-1)\n",
    "        pred_boxes[:, 3] = (coord[:, :, 3].detach().exp() * anchor_h).view(-1)\n",
    "        pred_boxes = pred_boxes.cpu()\n",
    "\n",
    "        # Get target values\n",
    "        coord_mask, conf_mask, cls_mask, tcoord, tconf, tcls = self.build_targets(pred_boxes, target, height, width)\n",
    "        coord_mask = coord_mask.expand_as(tcoord)\n",
    "        tcls = tcls[cls_mask].view(-1).long()\n",
    "        cls_mask = cls_mask.view(-1, 1).repeat(1, self.num_classes)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            tcoord = tcoord.cuda()\n",
    "            tconf = tconf.cuda()\n",
    "            coord_mask = coord_mask.cuda()\n",
    "            conf_mask = conf_mask.cuda()\n",
    "            tcls = tcls.cuda()\n",
    "            cls_mask = cls_mask.cuda()\n",
    "\n",
    "        conf_mask = conf_mask.sqrt()\n",
    "        cls = cls[cls_mask].view(-1, self.num_classes)\n",
    "\n",
    "        # Compute losses\n",
    "        mse = nn.MSELoss(size_average=False)\n",
    "        ce = nn.CrossEntropyLoss(size_average=False)\n",
    "        self.loss_coord = self.coord_scale * mse(coord * coord_mask, tcoord * coord_mask) / batch_size\n",
    "        self.loss_conf = mse(conf * conf_mask, tconf * conf_mask) / batch_size\n",
    "        self.loss_cls = self.class_scale * 2 * ce(cls, tcls) / batch_size\n",
    "        self.loss_tot = self.loss_coord + self.loss_conf + self.loss_cls\n",
    "\n",
    "        return self.loss_tot, self.loss_coord, self.loss_conf, self.loss_cls\n",
    "\n",
    "    def build_targets(self, pred_boxes, ground_truth, height, width):\n",
    "        batch_size = len(ground_truth)\n",
    "\n",
    "        conf_mask = torch.ones(batch_size, self.num_anchors, height * width, requires_grad=False) * self.noobject_scale\n",
    "        coord_mask = torch.zeros(batch_size, self.num_anchors, 1, height * width, requires_grad=False)\n",
    "        cls_mask = torch.zeros(batch_size, self.num_anchors, height * width, requires_grad=False).byte()\n",
    "        tcoord = torch.zeros(batch_size, self.num_anchors, 4, height * width, requires_grad=False)\n",
    "        tconf = torch.zeros(batch_size, self.num_anchors, height * width, requires_grad=False)\n",
    "        tcls = torch.zeros(batch_size, self.num_anchors, height * width, requires_grad=False)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            if len(ground_truth[b]) == 0:\n",
    "                continue\n",
    "\n",
    "            # Build up tensors\n",
    "            cur_pred_boxes = pred_boxes[\n",
    "                             b * (self.num_anchors * height * width):(b + 1) * (self.num_anchors * height * width)]\n",
    "            if self.anchor_step == 4:\n",
    "                anchors = self.anchors.clone()\n",
    "                anchors[:, :2] = 0\n",
    "            else:\n",
    "                anchors = torch.cat([torch.zeros_like(self.anchors), self.anchors], 1)\n",
    "            gt = torch.zeros(len(ground_truth[b]), 4)\n",
    "            for i, anno in enumerate(ground_truth[b]):\n",
    "                gt[i, 0] = (anno[0] + anno[2] / 2) / self.reduction\n",
    "                gt[i, 1] = (anno[1] + anno[3] / 2) / self.reduction\n",
    "                gt[i, 2] = anno[2] / self.reduction\n",
    "                gt[i, 3] = anno[3] / self.reduction\n",
    "\n",
    "            # Set confidence mask of matching detections to 0\n",
    "            iou_gt_pred = bbox_ious(gt, cur_pred_boxes)\n",
    "            mask = (iou_gt_pred > self.thresh).sum(0) >= 1\n",
    "            conf_mask[b][mask.view_as(conf_mask[b])] = 0\n",
    "\n",
    "            # Find best anchor for each ground truth\n",
    "            gt_wh = gt.clone()\n",
    "            gt_wh[:, :2] = 0\n",
    "            iou_gt_anchors = bbox_ious(gt_wh, anchors)\n",
    "            _, best_anchors = iou_gt_anchors.max(1)\n",
    "\n",
    "            # Set masks and target values for each ground truth\n",
    "            for i, anno in enumerate(ground_truth[b]):\n",
    "                gi = min(width - 1, max(0, int(gt[i, 0])))\n",
    "                gj = min(height - 1, max(0, int(gt[i, 1])))\n",
    "                best_n = best_anchors[i]\n",
    "                iou = iou_gt_pred[i][best_n * height * width + gj * width + gi]\n",
    "                coord_mask[b][best_n][0][gj * width + gi] = 1\n",
    "                cls_mask[b][best_n][gj * width + gi] = 1\n",
    "                conf_mask[b][best_n][gj * width + gi] = self.object_scale\n",
    "                tcoord[b][best_n][0][gj * width + gi] = gt[i, 0] - gi\n",
    "                tcoord[b][best_n][1][gj * width + gi] = gt[i, 1] - gj\n",
    "                tcoord[b][best_n][2][gj * width + gi] = math.log(max(gt[i, 2], 1.0) / self.anchors[best_n, 0])\n",
    "                tcoord[b][best_n][3][gj * width + gi] = math.log(max(gt[i, 3], 1.0) / self.anchors[best_n, 1])\n",
    "                tconf[b][best_n][gj * width + gi] = iou\n",
    "                tcls[b][best_n][gj * width + gi] = int(anno[4])\n",
    "\n",
    "        return coord_mask, conf_mask, cls_mask, tcoord, tconf, tcls\n",
    "\n",
    "\n",
    "def bbox_ious(boxes1, boxes2):\n",
    "    b1x1, b1y1 = (boxes1[:, :2] - (boxes1[:, 2:4] / 2)).split(1, 1)\n",
    "    b1x2, b1y2 = (boxes1[:, :2] + (boxes1[:, 2:4] / 2)).split(1, 1)\n",
    "    b2x1, b2y1 = (boxes2[:, :2] - (boxes2[:, 2:4] / 2)).split(1, 1)\n",
    "    b2x2, b2y2 = (boxes2[:, :2] + (boxes2[:, 2:4] / 2)).split(1, 1)\n",
    "\n",
    "    dx = (b1x2.min(b2x2.t()) - b1x1.max(b2x1.t())).clamp(min=0)\n",
    "    dy = (b1y2.min(b2y2.t()) - b1y1.max(b2y1.t())).clamp(min=0)\n",
    "    intersections = dx * dy\n",
    "\n",
    "    areas1 = (b1x2 - b1x1) * (b1y2 - b1y1)\n",
    "    areas2 = (b2x2 - b2x1) * (b2y2 - b2y1)\n",
    "    unions = (areas1 + areas2.t()) - intersections\n",
    "\n",
    "    return intersections / unions"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNltEs5A4yyzfmMnBCWAcsC",
   "collapsed_sections": [
    "uqmLN702siby"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
