{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKvr9DC6x9QM"
   },
   "source": [
    "# Object Detection\n",
    "* 2 people in a group\n",
    "* Deadline: 10/13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zPl8L7qx9QP"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "- PASCAL VOC 2007\n",
    "  - Number of class: 20\n",
    "  - The data list is provided in the google drive. However, you have to download the training/testing data from http://host.robots.ox.ac.uk/pascal/VOC/voc2007/. \n",
    "    - Train/Val data: 5011\n",
    "        - Each row contains one image and its bounding boxes.\n",
    "        - filename ($x_{min}$, $y_{min}$, $x_{max}$, $y_{max}$, $label$) $\\times$ object_num\n",
    "        - class idx starts from 1\n",
    "    - Test data: 4951\n",
    "        - filename ($x_{min}$, $y_{min}$, $x_{max}$, $y_{max}$, $label$) $\\times$ object_num\n",
    "        - class idx starts from 0\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXto_Mqqx9QQ"
   },
   "source": [
    "### Loading your data into Google Colab with Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZ4-AYFzx9QQ"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knUoH4P4x9QR"
   },
   "source": [
    "## Resnet50 backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ykAGukXsx9QS"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models.resnet import BasicBlock, Bottleneck\n",
    "from torchvision.models.resnet import model_urls\n",
    "\n",
    "class ResNetBackbone(nn.Module):\n",
    "\n",
    "    def __init__(self, resnet_type):\n",
    "\t\n",
    "        resnet_spec = {18: (BasicBlock, [2, 2, 2, 2], [64, 64, 128, 256, 512], 'resnet18'),\n",
    "\t\t       34: (BasicBlock, [3, 4, 6, 3], [64, 64, 128, 256, 512], 'resnet34'),\n",
    "\t\t       50: (Bottleneck, [3, 4, 6, 3], [64, 256, 512, 1024, 2048], 'resnet50'),\n",
    "\t\t       101: (Bottleneck, [3, 4, 23, 3], [64, 256, 512, 1024, 2048], 'resnet101'),\n",
    "\t\t       152: (Bottleneck, [3, 8, 36, 3], [64, 256, 512, 1024, 2048], 'resnet152')}\n",
    "        block, layers, channels, name = resnet_spec[resnet_type]\n",
    "        \n",
    "        self.name = name\n",
    "        self.inplanes = 64\n",
    "        super(ResNetBackbone, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.001)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.layer2(x1)\n",
    "        x3 = self.layer3(x2)\n",
    "        x4 = self.layer4(x3)\n",
    "        # x4 layer output size: (B, 2048, 8, 8)\n",
    "        return x4\n",
    "\n",
    "    def init_weights(self):\n",
    "        org_resnet = torch.utils.model_zoo.load_url(model_urls[self.name])\n",
    "        # drop orginal resnet fc layer, add 'None' in case of no fc layer, that will raise error\n",
    "        org_resnet.pop('fc.weight', None)\n",
    "        org_resnet.pop('fc.bias', None)\n",
    "\n",
    "        self.load_state_dict(org_resnet)\n",
    "        print(\"Initialize resnet from model zoo\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kj7b4Idx9QT"
   },
   "source": [
    "### Assignment\n",
    "You are required to build a model to perform object detection on the provided Pascal VOC dataset in this project.\n",
    "Here are some hints that help you to accomplish the project successfully.\n",
    "\n",
    "### Hints\n",
    "- YOLOv1 is the simplest and suggested model to be implemented.\n",
    "- Be careful of the normalization techniques on bounding boxes.\n",
    "    1. normalize the height and width with image size to fall into 0 and 1\n",
    "    2. x and y coordinates are parameterized to be the offsets of a particular grid cell and also bounded by 0 and 1\n",
    "- Loss function has a great impact on training stability.\n",
    "    1. loss function is the most important in this project, especially in calculating IOU\n",
    "    2. only one bounding box predictor is responsible for each object\n",
    "    3. weights for different types of losses\n",
    "    4. predict the square root of height and width instead of predicting them directly\n",
    "- Data augmentation.\n",
    "    1. It contains only 5011 images in total. Furthermore, the labels are highly imbalanced.\n",
    "    2. Random scaling and translations are applied when training YOLO.\n",
    "    3. Note that the bounding box coordinates have to be changed accordingly if the image was transformed.\n",
    "\n",
    "### Evaluation Metric\n",
    "- Please evaluate your model on Pascal VOC testing set using Mean Average Precision (mAP).\n",
    "- Write a brief report including your implementation, performance and  qualitative results(visualize bounding box on some images). \n",
    "- For more detailed explanation of mAP, please follow https://github.com/rafaelpadilla/Object-Detection-Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ojEJprqbx9QU"
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class yolov1Loss(nextn.Module):\n",
    "    def __init__(self, S, B, C, lambda_coord, lambda_noobj):\n",
    "        # Args:\n",
    "        #    S: size of grid\n",
    "        #    B: number of box\n",
    "        #    C: number of class\n",
    "        super(yolov1Loss, self).__init__()\n",
    "        self.S = S \n",
    "        self.B = B \n",
    "        self.C = C \n",
    "        self.l_coord = lambda_coord\n",
    "        self.l_noobj = lambda_noobj\n",
    "    \n",
    "    def calculateIoU(self, box1, box2):\n",
    "        # calculate the intersection over the union of two sets of boxes, each box contains [xmin,ymin,xmax,ymax]\n",
    "        # Args:\n",
    "        #    size of box1 = [n,4]\n",
    "        #    size of box2 = [m,4]\n",
    "        # Return:\n",
    "        #    size of Iou of two sets of boxes = [n,m]\n",
    "        n = box1.size(0)\n",
    "        m = box2.size(0)\n",
    "        \n",
    "        # take the max of left-bottom point and the min of right-top point \n",
    "        # to calculate the left-top point and the right-bottom point of the intersection\n",
    "        lt = torch.max(\n",
    "            box1[:,:2].unsqueeze(1).expand(n,m,2), # take [xmin,ymin]: [n,2] -> [n,1,2] -> [n,m,2]\n",
    "            box2[:,:2].unsqueeze(0).expand(n,m,2)  # take [xmin,ymin]: [m,2] -> [1,m,2] -> [n,m,2]\n",
    "        )\n",
    "        \n",
    "        rb = torch.min(\n",
    "            box1[:,2:].unsqueeze(1).expand(n,m,2), # take [xmax,ymax]: [n,2] -> [n,1,2] -> [n,m,2]\n",
    "            box2[:,2:].unsqueeze(0).expand(n,m,2)  # take [xmax,ymax]: [m,2] -> [1,m,2] -> [n,m,2]\n",
    "        )\n",
    "        \n",
    "        # calculate weight and height of intersection areas and check if intersection area is 0\n",
    "        wh = rb - lt # [n,m,2]\n",
    "        wh[wh<0] = 0 # if max_left >= min_right or max_bottom >= min_top, then there is no intersection\n",
    "        intersection = wh[:,:,0] * wh[:,:,1] # [n,m]\n",
    "        \n",
    "        area1 = (box1[:,2]-box1[:,0])*(box1[:,3]-box1[:,1])  #[n,]     \n",
    "        area2 = (box2[:,2]-box2[:,0])*(box2[:,3]-box2[:,1])  #[m,]\n",
    "        area1 = area1.unsqueeze(1).expand(n,m) # [n,] -> [n,1] -> [n,m]\n",
    "        area2 = area2.unsqueeze(0).expand(n,m) # [m,] -> [1,m] -> [n,m]\n",
    "        \n",
    "        iou = intersection / (area1 + area2 - intersection)\n",
    "        return iou\n",
    "    def farword(self, preds, targets):\n",
    "        # Args:\n",
    "        #    size of preds = [batchsize, S, S, Bx5+20]: Bx5 means each box has [x,y,w,h,c] 5 values\n",
    "        #    size of targets = [batchsize, S, S, Bx5+20]\n",
    "        S, B, C = self.S, self.B, self.C\n",
    "        N = B * 5 + C \n",
    "        batchsize = preds.size(0)\n",
    "        coord_mask = targets[:,:,:,4] > 0        \n",
    "        noobj_mask = targets[:,:,:,4] == 0\n",
    "        coord_mask = coord_mask.unsqueeze(-1).expand(batchsize, S, S, N)        \n",
    "        noobj_mask = noobj_mask.unsqueeze(-1).expand(batchsize, S, S, N)\n",
    "        \n",
    "        coord_pred = preds[coord_mask].view(-1, N)\n",
    "        box_pred = coord_pred[:,:5*B].contiguous().view(-1, 5)\n",
    "        class_pred = coord_pred[:,5*B:]\n",
    "        \n",
    "        coord_target = targets[coord_mask].view(-1, N)\n",
    "        box_target = coord_target[:,:5*B].contiguous().view(-1, 5)\n",
    "        class_target = coord_target[:,5*B:]\n",
    "        \n",
    "        # compute noobj_loss: only calculate confidence loss\n",
    "        noobj_pred = preds[noobj_mask].view(-1, N)\n",
    "        noobj_target = targets[noobj_mask].view(-1, N)\n",
    "        noobj_pred_mask = torch.cuda.ByteTensor(noobj_pred.size())\n",
    "        noobj_pred_mask.zero_()\n",
    "        for b in range(B):\n",
    "            noobj_pred_mask[:, 4+b*5] = 1\n",
    "        noobj_pred_conf = noobj_pred[noobj_pred_mask]\n",
    "        noobj_target_conf = noobj_target[noobj_pred_mask]  \n",
    "        loss_noobj = F.mse_loss(noobj_pred_conf, noobj_target_conf, reduction = 'sum')\n",
    "        \n",
    "        # compute coord_loss\n",
    "        coord_response_mask = torch.cuda.ByteTensor(box_target.size()).fill_(0) # only compute the loss of the box containing the center of object\n",
    "        box_target_iou = torch.zero(box_target.size()).cuda()\n",
    "        \n",
    "        # Choose the pred box having the highest IoU for each target boxes\n",
    "        for i in range(0, box_target.size(0), B):\n",
    "            # take all predict boxes at i-th cell\n",
    "            pred_boxes = box_pred[i:i+B]\n",
    "            pred_xyxy = Variable(torch.FloatTensor(pred_boxes.size()))\n",
    "            pred_xyxy[:, :2] = pred_boxes[:, :2]/float(S) - 0.5*pred_boxes[:,2:4]\n",
    "            pred_xyxy[:, 2:4] = pred_boxes[:, :2]/float(S) + 0.5*pred_boxes[:,2:4]   \n",
    "            \n",
    "            # take all target boxes at i-th cell\n",
    "            # Since target boxes contained by each cell are identical in current implement,thus just take the first one\n",
    "            target_boxes = box_target[i].view(-1, 5)\n",
    "            target_xyxy = Variable(torch.FloatTensor(target_boxes.size())\n",
    "            target_xyxy[:, :2] = target_boxes[:, :2]/float(S) - 0.5*target_boxes[:,2:4]\n",
    "            target_xyxy[:, 2:4] = target_boxes[:, :2]/float(S) + 0.5*target_boxes[:,2:4]\n",
    "                                   \n",
    "            iou = self.calculateIoU(pred_xyxy[:,:4], target_xyxy[:,:4]) # [B,1]\n",
    "            max_iou, max_index = iou.max(0)\n",
    "            max_index = max_index.data.cuda()\n",
    "            \n",
    "            coord_response_mask[i+max_index] = 1\n",
    "            box_target_iou[i+max_index, torch.LongTensor([4]).cuda()] = (max_iou).data.cuda()\n",
    "        \n",
    "        # calculate the loss of the response boxes\n",
    "        box_target_iou = Variable(box_target_iou).cuda()\n",
    "        box_pred_response = box_pred[coord_response_mask].view(-1, 5)\n",
    "        box_target_response = box_target[coord_response_mask].view(-1, 5)\n",
    "        target_iou = box_target_iou[coord_response_mask].view(-1, 5)\n",
    "        loss_xy = F.mse_loss(box_pred_response[:,:2], box_target_response[:,:2], reduction = 'sum')\n",
    "        loss_wh = F.mse_loss(torch.sqrt(box_pred_response[:,2:4]), torch.sqrt(box_target_response[:,2:4]), reduction = 'sum')                \n",
    "        loss_obj = F. mse_loss(box_pred_response[:,4], target_iou[:,4], reduction = 'sum')\n",
    "        \n",
    "        # calculate the class probability loss of cells containing objects\n",
    "        loss_class = F.mse_loss(class_pred, class_target, resuction = 'sum')\n",
    "        \n",
    "        # total loss\n",
    "        loss = self.l_coord * (loss_xy + loss_wh) + loss_obj + self.l_noobj*loss_noobj + loss_class\n",
    "        loss = loss/float(batchsize)\n",
    "                                   \n",
    "        return loss"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1TP-dps8rtpZ2G9mVvXEF7WXBuUV1oJFZ",
     "timestamp": 1662903786776
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
